{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Tce3stUlHN0L"
   },
   "source": [
    "##### Copyright 2020 The TensorFlow Authors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "cellView": "form",
    "id": "tuOe1ymfHZPu"
   },
   "outputs": [],
   "source": [
    "#@title Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "# https://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qFdPvlXBOdUN"
   },
   "source": [
    "# Introduction to gradients and automatic differentiation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MfBg1C5NB3X0"
   },
   "source": [
    "<table class=\"tfo-notebook-buttons\" align=\"left\">\n",
    "  <td>\n",
    "    <a target=\"_blank\" href=\"https://www.tensorflow.org/guide/autodiff\"><img src=\"https://www.tensorflow.org/images/tf_logo_32px.png\" />View on TensorFlow.org</a>\n",
    "  </td>\n",
    "  <td>\n",
    "    <a target=\"_blank\" href=\"https://colab.research.google.com/github/tensorflow/docs/blob/master/site/en/guide/autodiff.ipynb\"><img src=\"https://www.tensorflow.org/images/colab_logo_32px.png\" />Run in Google Colab</a>\n",
    "  </td>\n",
    "  <td>\n",
    "    <a target=\"_blank\" href=\"https://github.com/tensorflow/docs/blob/master/site/en/guide/autodiff.ipynb\"><img src=\"https://www.tensorflow.org/images/GitHub-Mark-32px.png\" />View source on GitHub</a>\n",
    "  </td>\n",
    "  <td>\n",
    "    <a href=\"https://storage.googleapis.com/tensorflow_docs/docs/site/en/guide/autodiff.ipynb\"><img src=\"https://www.tensorflow.org/images/download_logo_32px.png\" />Download notebook</a>\n",
    "  </td>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "r6P32iYYV27b"
   },
   "source": [
    "## Automatic Differentiation and Gradients\n",
    "\n",
    "[Automatic differentiation](https://en.wikipedia.org/wiki/Automatic_differentiation)\n",
    "is useful for implementing machine learning algorithms such as\n",
    "[backpropagation](https://en.wikipedia.org/wiki/Backpropagation) for training\n",
    "neural networks.\n",
    "\n",
    "In this guide, you will explore ways to compute gradients with TensorFlow, especially in [eager execution](eager.ipynb)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MUXex9ctTuDB"
   },
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "IqR2PQG4ZaZ0"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xHxb-dlhMIzW"
   },
   "source": [
    "## Computing gradients\n",
    "\n",
    "To differentiate automatically, TensorFlow needs to remember what operations happen in what order during the *forward* pass.  Then, during the *backward pass*, TensorFlow traverses this list of operations in reverse order to compute gradients."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1CLWJl0QliB0"
   },
   "source": [
    "## Gradient tapes\n",
    "\n",
    "TensorFlow provides the `tf.GradientTape` API for automatic differentiation; that is, computing the gradient of a computation with respect to some inputs, usually `tf.Variable`s.\n",
    "TensorFlow \"records\" relevant operations executed inside the context of a `tf.GradientTape` onto a \"tape\". TensorFlow then uses that tape to compute the gradients of a \"recorded\" computation using [reverse mode differentiation](https://en.wikipedia.org/wiki/Automatic_differentiation).\n",
    "\n",
    "Here is a simple example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = tf.Variable( 5.)\n",
    "\n",
    "with tf.GradientTape() as tape:\n",
    "    y = x**3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CR9tFAP_7cra"
   },
   "source": [
    "Once you've recorded some operations, use `GradientTape.gradient(target, sources)` to calculate the gradient of some target (often a loss) relative to some source (often the model's variables):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "74.99999"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dy_dx = tape.gradient(y, x)\n",
    "dy_dx.numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Q2_aqsO25Vx1"
   },
   "source": [
    "The above example uses scalars, but `tf.GradientTape` works as easily on any tensor:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 3)\n"
     ]
    }
   ],
   "source": [
    "w = tf.Variable(tf.random.normal((3,2)), name = 'wt')\n",
    "b = tf.Variable(tf.ones(2, dtype=tf.float32), name= 'b')\n",
    "x = tf.constant( [1.,2.,3.] )\n",
    "\n",
    "x = x[tf.newaxis, :]\n",
    "print(x.shape)\n",
    "\n",
    "\n",
    "with tf.GradientTape(persistent=True) as tape:\n",
    "    y = x@w + b\n",
    "    loss = tf.reduce_mean(y**2)\n",
    "\n",
    "\n",
    "if 0:\n",
    "    def grad_fn(w,b,x):\n",
    "        with tf.GradientTape() as tape:\n",
    "            y = x@w + b\n",
    "            loss = tf.reduce_mean(y**2)\n",
    "\n",
    "        return tape.gradient(loss, [w,b])\n",
    "\n",
    "    grad_fn(w,b,x)\n",
    "    grad_fn(w,b,x)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "i4eXOkrQ-9Pb"
   },
   "source": [
    "To get the gradient of `loss` with respect to both variables, you can pass both as sources to the `gradient` method. The tape is flexible about how sources are passed and will accept any nested combination of lists or dictionaries and return the gradient structured the same way (see `tf.nest`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# both the below are equivalent \n",
    "# [dl_dw, dl_db] = tape.gradient(loss, [w,b])\n",
    "grads = tape.gradient(loss, [w,b])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<tf.Tensor: shape=(3, 2), dtype=float32, numpy=\n",
       " array([[0.06396484, 1.1484375 ],\n",
       "        [0.12792969, 2.296875  ],\n",
       "        [0.19189453, 3.4453125 ]], dtype=float32)>,\n",
       " <tf.Tensor: shape=(2,), dtype=float32, numpy=array([0.06396484, 1.1480713 ], dtype=float32)>]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# dl_dw, dl_db\n",
    "grads"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ei4iVXi6qgM7"
   },
   "source": [
    "The gradient with respect to each source has the shape of the source:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "aYbWRFPZqk4U"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3, 2)\n",
      "(3, 2)\n"
     ]
    }
   ],
   "source": [
    "print(w.shape)\n",
    "print(grads[0].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dI_SzxHsvao1"
   },
   "source": [
    "Here is the gradient calculation again, this time passing a dictionary of variables:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "d73cY6NOuaMd"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(2,), dtype=float32, numpy=array([0.06396484, 1.1480713 ], dtype=float32)>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_vars = {\n",
    "    'w': w,\n",
    "    'b': b\n",
    "}\n",
    "\n",
    "grad = tape.gradient(loss, my_vars)\n",
    "grad['b']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HZ2LvHifEMgO"
   },
   "source": [
    "## Gradients with respect to a model\n",
    "\n",
    "It's common to collect `tf.Variables` into a `tf.Module` or one of its subclasses (`layers.Layer`, `keras.Model`) for [checkpointing](checkpoint.ipynb) and [exporting](saved_model.ipynb).\n",
    "\n",
    "In most cases, you will want to calculate gradients with respect to a model's trainable variables.  Since all subclasses of `tf.Module` aggregate their variables in the `Module.trainable_variables` property, you can calculate these gradients in a few lines of code: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Original Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "layer = tf.keras.layers.Dense(2, activation='relu')\n",
    "x = tf.constant([[1., 2., 3.]])\n",
    "\n",
    "with tf.GradientTape() as tape:\n",
    "  # Forward pass\n",
    "  y = layer(x)\n",
    "  loss = tf.reduce_mean(y**2)\n",
    "\n",
    "# Calculate gradients with respect to every trainable variable\n",
    "grad = tape.gradient(loss, layer.trainable_variables)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### My version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.06396484, 1.1480713 ], dtype=float32)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grads[0].numpy()\n",
    "grads[1].numpy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n",
      "\n",
      "layer.trainable_variables =  []\n",
      "\n",
      "\n",
      "\n",
      " current loss =  tf.Tensor(0.41396075, shape=(), dtype=float32)\n",
      "\n",
      "grad =  [<tf.Tensor: shape=(3, 20), dtype=float32, numpy=\n",
      "array([[0.        , 0.        , 0.08068848, 0.        , 0.        ,\n",
      "        0.        , 0.        , 0.        , 0.        , 0.06088257,\n",
      "        0.        , 0.0163269 , 0.13269043, 0.07989502, 0.18054199,\n",
      "        0.08966064, 0.        , 0.        , 0.01305389, 0.08679199],\n",
      "       [0.        , 0.        , 0.16137695, 0.        , 0.        ,\n",
      "        0.        , 0.        , 0.        , 0.        , 0.12176514,\n",
      "        0.        , 0.03265381, 0.26538086, 0.15979004, 0.36108398,\n",
      "        0.17932129, 0.        , 0.        , 0.02610779, 0.17358398],\n",
      "       [0.        , 0.        , 0.24206543, 0.        , 0.        ,\n",
      "        0.        , 0.        , 0.        , 0.        , 0.1826477 ,\n",
      "        0.        , 0.04898071, 0.3980713 , 0.23968506, 0.541626  ,\n",
      "        0.26898193, 0.        , 0.        , 0.03916168, 0.26037598]],\n",
      "      dtype=float32)>, <tf.Tensor: shape=(20,), dtype=float32, numpy=\n",
      "array([0.        , 0.        , 0.08066406, 0.        , 0.        ,\n",
      "       0.        , 0.        , 0.        , 0.        , 0.06088867,\n",
      "       0.        , 0.0163208 , 0.13266602, 0.07988892, 0.18052979,\n",
      "       0.08964843, 0.        , 0.        , 0.01305542, 0.08678055],\n",
      "      dtype=float32)>]\n",
      "\n",
      "layer.trainable_variables =  [<tf.Variable 'dense_2/kernel:0' shape=(3, 20) dtype=float32, numpy=\n",
      "array([[-0.23718464,  0.02653927,  0.40243334, -0.4252332 ,  0.1129275 ,\n",
      "        -0.26490915,  0.06161934, -0.01104349,  0.39263332, -0.31697398,\n",
      "        -0.269037  , -0.4865227 ,  0.40665242, -0.16798016, -0.23885487,\n",
      "        -0.33318678, -0.06444216, -0.33513647,  0.05247743, -0.01343237],\n",
      "       [-0.4950554 , -0.15553084, -0.21532458, -0.07865468,  0.46069783,\n",
      "        -0.14455894,  0.28812468, -0.06352666, -0.01248345, -0.16176799,\n",
      "        -0.4600847 ,  0.43452972,  0.2912618 ,  0.29595262,  0.38343337,\n",
      "        -0.13669635, -0.25541082,  0.16872448,  0.37571052, -0.32222778],\n",
      "       [ 0.2014482 ,  0.0205552 ,  0.24075541, -0.05815208, -0.41788223,\n",
      "         0.11015725, -0.31370378, -0.34690684, -0.14074427,  0.38803536,\n",
      "        -0.17924166, -0.08068061,  0.05052913,  0.08765055,  0.34151396,\n",
      "         0.45915097, -0.2604062 , -0.4340698 , -0.23055041,  0.4681205 ]],\n",
      "      dtype=float32)>, <tf.Variable 'dense_2/bias:0' shape=(20,) dtype=float32, numpy=\n",
      "array([ 0.        ,  0.        , -0.00806641,  0.        ,  0.        ,\n",
      "        0.        ,  0.        ,  0.        ,  0.        , -0.00608887,\n",
      "        0.        , -0.00163208, -0.0132666 , -0.00798889, -0.01805298,\n",
      "       -0.00896484,  0.        ,  0.        , -0.00130554, -0.00867806],\n",
      "      dtype=float32)>]\n",
      "\n",
      "\n",
      "\n",
      " current loss =  tf.Tensor(0.29909182, shape=(), dtype=float32)\n",
      "\n",
      "grad =  [<tf.Tensor: shape=(3, 20), dtype=float32, numpy=\n",
      "array([[0.        , 0.        , 0.06860352, 0.        , 0.        ,\n",
      "        0.        , 0.        , 0.        , 0.        , 0.05172729,\n",
      "        0.        , 0.0138855 , 0.11273193, 0.06787109, 0.15344238,\n",
      "        0.07623291, 0.        , 0.        , 0.01108551, 0.07373047],\n",
      "       [0.        , 0.        , 0.13720703, 0.        , 0.        ,\n",
      "        0.        , 0.        , 0.        , 0.        , 0.10345459,\n",
      "        0.        , 0.027771  , 0.22546387, 0.13574219, 0.30688477,\n",
      "        0.15246582, 0.        , 0.        , 0.02217102, 0.14746094],\n",
      "       [0.        , 0.        , 0.20581055, 0.        , 0.        ,\n",
      "        0.        , 0.        , 0.        , 0.        , 0.15518188,\n",
      "        0.        , 0.04165649, 0.3381958 , 0.20361328, 0.46032715,\n",
      "        0.22869873, 0.        , 0.        , 0.03325653, 0.2211914 ]],\n",
      "      dtype=float32)>, <tf.Tensor: shape=(20,), dtype=float32, numpy=\n",
      "array([0.        , 0.        , 0.06857812, 0.        , 0.        ,\n",
      "       0.        , 0.        , 0.        , 0.        , 0.05173486,\n",
      "       0.        , 0.01388708, 0.11276026, 0.06787787, 0.15348035,\n",
      "       0.07620312, 0.        , 0.        , 0.01108771, 0.0737408 ],\n",
      "      dtype=float32)>]\n",
      "\n",
      "layer.trainable_variables =  [<tf.Variable 'dense_2/kernel:0' shape=(3, 20) dtype=float32, numpy=\n",
      "array([[-0.23718464,  0.02653927,  0.395573  , -0.4252332 ,  0.1129275 ,\n",
      "        -0.26490915,  0.06161934, -0.01104349,  0.39263332, -0.3221467 ,\n",
      "        -0.269037  , -0.48791125,  0.39537922, -0.17476727, -0.25419912,\n",
      "        -0.34081006, -0.06444216, -0.33513647,  0.05136888, -0.02080542],\n",
      "       [-0.4950554 , -0.15553084, -0.22904529, -0.07865468,  0.46069783,\n",
      "        -0.14455894,  0.28812468, -0.06352666, -0.01248345, -0.17211345,\n",
      "        -0.4600847 ,  0.43175262,  0.2687154 ,  0.2823784 ,  0.3527449 ,\n",
      "        -0.15194294, -0.25541082,  0.16872448,  0.3734934 , -0.33697388],\n",
      "       [ 0.2014482 ,  0.0205552 ,  0.22017436, -0.05815208, -0.41788223,\n",
      "         0.11015725, -0.31370378, -0.34690684, -0.14074427,  0.37251717,\n",
      "        -0.17924166, -0.08484626,  0.01670955,  0.06728923,  0.29548123,\n",
      "         0.43628109, -0.2604062 , -0.4340698 , -0.23387606,  0.44600135]],\n",
      "      dtype=float32)>, <tf.Variable 'dense_2/bias:0' shape=(20,) dtype=float32, numpy=\n",
      "array([ 0.        ,  0.        , -0.01492422,  0.        ,  0.        ,\n",
      "        0.        ,  0.        ,  0.        ,  0.        , -0.01126235,\n",
      "        0.        , -0.00302079, -0.02454263, -0.01477668, -0.03340101,\n",
      "       -0.01658516,  0.        ,  0.        , -0.00241431, -0.01605214],\n",
      "      dtype=float32)>]\n",
      "\n",
      "\n",
      "\n",
      " current loss =  tf.Tensor(0.21610777, shape=(), dtype=float32)\n",
      "\n",
      "grad =  [<tf.Tensor: shape=(3, 20), dtype=float32, numpy=\n",
      "array([[0.        , 0.        , 0.05831909, 0.        , 0.        ,\n",
      "        0.        , 0.        , 0.        , 0.        , 0.04397583,\n",
      "        0.        , 0.01179504, 0.0958252 , 0.05770874, 0.1303711 ,\n",
      "        0.0647583 , 0.        , 0.        , 0.00943756, 0.06274414],\n",
      "       [0.        , 0.        , 0.11663818, 0.        , 0.        ,\n",
      "        0.        , 0.        , 0.        , 0.        , 0.08795166,\n",
      "        0.        , 0.02359009, 0.19165039, 0.11541748, 0.2607422 ,\n",
      "        0.1295166 , 0.        , 0.        , 0.01887512, 0.12548828],\n",
      "       [0.        , 0.        , 0.17495728, 0.        , 0.        ,\n",
      "        0.        , 0.        , 0.        , 0.        , 0.13192749,\n",
      "        0.        , 0.03538513, 0.2874756 , 0.17312622, 0.39111328,\n",
      "        0.1942749 , 0.        , 0.        , 0.02831268, 0.18823242]],\n",
      "      dtype=float32)>, <tf.Tensor: shape=(20,), dtype=float32, numpy=\n",
      "array([0.        , 0.        , 0.05832203, 0.        , 0.        ,\n",
      "       0.        , 0.        , 0.        , 0.        , 0.04399095,\n",
      "       0.        , 0.01179509, 0.09584438, 0.05771422, 0.13042454,\n",
      "       0.06474774, 0.        , 0.        , 0.00943569, 0.06274416],\n",
      "      dtype=float32)>]\n",
      "\n",
      "layer.trainable_variables =  [<tf.Variable 'dense_2/kernel:0' shape=(3, 20) dtype=float32, numpy=\n",
      "array([[-0.23718464,  0.02653927,  0.3897411 , -0.4252332 ,  0.1129275 ,\n",
      "        -0.26490915,  0.06161934, -0.01104349,  0.39263332, -0.32654428,\n",
      "        -0.269037  , -0.48909077,  0.3857967 , -0.18053815, -0.26723623,\n",
      "        -0.3472859 , -0.06444216, -0.33513647,  0.05042512, -0.02707983],\n",
      "       [-0.4950554 , -0.15553084, -0.24070911, -0.07865468,  0.46069783,\n",
      "        -0.14455894,  0.28812468, -0.06352666, -0.01248345, -0.18090862,\n",
      "        -0.4600847 ,  0.42939362,  0.24955037,  0.27083665,  0.32667068,\n",
      "        -0.1648946 , -0.25541082,  0.16872448,  0.3716059 , -0.3495227 ],\n",
      "       [ 0.2014482 ,  0.0205552 ,  0.20267864, -0.05815208, -0.41788223,\n",
      "         0.11015725, -0.31370378, -0.34690684, -0.14074427,  0.35932443,\n",
      "        -0.17924166, -0.08838477, -0.01203801,  0.0499766 ,  0.25636992,\n",
      "         0.4168536 , -0.2604062 , -0.4340698 , -0.23670733,  0.4271781 ]],\n",
      "      dtype=float32)>, <tf.Variable 'dense_2/bias:0' shape=(20,) dtype=float32, numpy=\n",
      "array([ 0.        ,  0.        , -0.02075642,  0.        ,  0.        ,\n",
      "        0.        ,  0.        ,  0.        ,  0.        , -0.01566145,\n",
      "        0.        , -0.0042003 , -0.03412707, -0.0205481 , -0.04644347,\n",
      "       -0.02305993,  0.        ,  0.        , -0.00335788, -0.02232655],\n",
      "      dtype=float32)>]\n",
      "\n",
      "\n",
      "\n",
      " current loss =  tf.Tensor(0.15607783, shape=(), dtype=float32)\n",
      "\n",
      "grad =  [<tf.Tensor: shape=(3, 20), dtype=float32, numpy=\n",
      "array([[0.        , 0.        , 0.04953003, 0.        , 0.        ,\n",
      "        0.        , 0.        , 0.        , 0.        , 0.03738403,\n",
      "        0.        , 0.01005554, 0.08148193, 0.04904175, 0.11083984,\n",
      "        0.05502319, 0.        , 0.        , 0.00801086, 0.05331421],\n",
      "       [0.        , 0.        , 0.09906006, 0.        , 0.        ,\n",
      "        0.        , 0.        , 0.        , 0.        , 0.07476807,\n",
      "        0.        , 0.02011108, 0.16296387, 0.0980835 , 0.22167969,\n",
      "        0.11004639, 0.        , 0.        , 0.01602173, 0.10662842],\n",
      "       [0.        , 0.        , 0.14859009, 0.        , 0.        ,\n",
      "        0.        , 0.        , 0.        , 0.        , 0.1121521 ,\n",
      "        0.        , 0.03016663, 0.2444458 , 0.14712524, 0.33251953,\n",
      "        0.16506958, 0.        , 0.        , 0.02403259, 0.15994263]],\n",
      "      dtype=float32)>, <tf.Tensor: shape=(20,), dtype=float32, numpy=\n",
      "array([0.        , 0.        , 0.04953569, 0.        , 0.        ,\n",
      "       0.        , 0.        , 0.        , 0.        , 0.0373987 ,\n",
      "       0.        , 0.0100536 , 0.08145209, 0.04903772, 0.11085858,\n",
      "       0.05501823, 0.        , 0.        , 0.00801382, 0.05331087],\n",
      "      dtype=float32)>]\n",
      "\n",
      "layer.trainable_variables =  [<tf.Variable 'dense_2/kernel:0' shape=(3, 20) dtype=float32, numpy=\n",
      "array([[-0.23718464,  0.02653927,  0.3847881 , -0.4252332 ,  0.1129275 ,\n",
      "        -0.26490915,  0.06161934, -0.01104349,  0.39263332, -0.3302827 ,\n",
      "        -0.269037  , -0.49009633,  0.3776485 , -0.18544233, -0.27832022,\n",
      "        -0.3527882 , -0.06444216, -0.33513647,  0.04962404, -0.03241125],\n",
      "       [-0.4950554 , -0.15553084, -0.25061512, -0.07865468,  0.46069783,\n",
      "        -0.14455894,  0.28812468, -0.06352666, -0.01248345, -0.18838543,\n",
      "        -0.4600847 ,  0.4273825 ,  0.23325399,  0.2610283 ,  0.3045027 ,\n",
      "        -0.17589924, -0.25541082,  0.16872448,  0.37000373, -0.36018556],\n",
      "       [ 0.2014482 ,  0.0205552 ,  0.18781963, -0.05815208, -0.41788223,\n",
      "         0.11015725, -0.31370378, -0.34690684, -0.14074427,  0.34810922,\n",
      "        -0.17924166, -0.09140144, -0.03648259,  0.03526408,  0.22311796,\n",
      "         0.40034664, -0.2604062 , -0.4340698 , -0.23911059,  0.41118386]],\n",
      "      dtype=float32)>, <tf.Variable 'dense_2/bias:0' shape=(20,) dtype=float32, numpy=\n",
      "array([ 0.        ,  0.        , -0.02570999,  0.        ,  0.        ,\n",
      "        0.        ,  0.        ,  0.        ,  0.        , -0.01940132,\n",
      "        0.        , -0.00520566, -0.04227228, -0.02545187, -0.05752932,\n",
      "       -0.02856175,  0.        ,  0.        , -0.00415926, -0.02765764],\n",
      "      dtype=float32)>]\n",
      "\n",
      "\n",
      "\n",
      " current loss =  tf.Tensor(0.112828866, shape=(), dtype=float32)\n",
      "\n",
      "grad =  [<tf.Tensor: shape=(3, 20), dtype=float32, numpy=\n",
      "array([[0.        , 0.        , 0.04211426, 0.        , 0.        ,\n",
      "        0.        , 0.        , 0.        , 0.        , 0.03179932,\n",
      "        0.        , 0.00855255, 0.0692749 , 0.04168701, 0.09423828,\n",
      "        0.04681396, 0.        , 0.        , 0.00682831, 0.0453186 ],\n",
      "       [0.        , 0.        , 0.08422852, 0.        , 0.        ,\n",
      "        0.        , 0.        , 0.        , 0.        , 0.06359863,\n",
      "        0.        , 0.0171051 , 0.1385498 , 0.08337402, 0.18847656,\n",
      "        0.09362793, 0.        , 0.        , 0.01365662, 0.09063721],\n",
      "       [0.        , 0.        , 0.12634277, 0.        , 0.        ,\n",
      "        0.        , 0.        , 0.        , 0.        , 0.09539795,\n",
      "        0.        , 0.02565765, 0.2078247 , 0.12506104, 0.28271484,\n",
      "        0.1404419 , 0.        , 0.        , 0.02048492, 0.13595581]],\n",
      "      dtype=float32)>, <tf.Tensor: shape=(20,), dtype=float32, numpy=\n",
      "array([0.        , 0.        , 0.04211894, 0.        , 0.        ,\n",
      "       0.        , 0.        , 0.        , 0.        , 0.0318001 ,\n",
      "       0.        , 0.00854926, 0.06925605, 0.0416931 , 0.09424707,\n",
      "       0.04680203, 0.        , 0.        , 0.00682895, 0.04531163],\n",
      "      dtype=float32)>]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import tensorflow as tf\n",
    "tf.compat.v1.enable_eager_execution()\n",
    "\n",
    "# imporvement in loss is visible on high number of params(weights). thus, num neurons made 20.\n",
    "layer2 = tf.keras.layers.Dense(20, activation='relu')\n",
    "\n",
    "x2 = tf.constant( [[1.,2.,3.]] )\n",
    "\n",
    "optimizer = tf.keras.optimizers.SGD(learning_rate=0.1)\n",
    "\n",
    "def compute_loss(layer, x):\n",
    "    y = layer(x, training=True)\n",
    "    loss = tf.reduce_mean(y**2.)\n",
    "    return loss\n",
    "\n",
    "def grad_fn_2(layer, x):\n",
    "    with tf.GradientTape() as tape:\n",
    "        tape.watch(layer.trainable_variables)\n",
    "        loss = compute_loss(layer, x)\n",
    "    return tape.gradient( loss, layer.trainable_variables )\n",
    "\n",
    "print(layer2.trainable_variables)\n",
    "\n",
    "# Calculate gradients with respect to every trainable variable\n",
    "for i in range(5):\n",
    "    layer2.trainable = True\n",
    "    print(\"\\nlayer.trainable_variables = \", layer2.trainable_variables)\n",
    "    current_loss = compute_loss(layer2, x2)\n",
    "    print(\"\\n\\n\\n current loss = \", current_loss )\n",
    "    grads = grad_fn_2(layer2, x2)\n",
    "    optimizer.apply_gradients( zip(grads, layer2.trainable_variables) )\n",
    "    if i == 0:\n",
    "        w_test = layer2.trainable_variables[0].numpy()\n",
    "        b_test = layer2.trainable_variables[1].numpy()\n",
    "        loss_test = current_loss\n",
    "    print(\"\\ngrad = \", grads)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "PR_ezr6UFrpI"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dense_1/kernel:0, shape: (3, 2)\n",
      "dense_1/bias:0, shape: (2,)\n"
     ]
    }
   ],
   "source": [
    "for var, g in zip(layer.trainable_variables, grad):\n",
    "  print(f'{var.name}, shape: {g.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<tf.Variable 'dense_1/kernel:0' shape=(3, 2) dtype=float32, numpy=\n",
       " array([[-1.0683123 ,  0.6282842 ],\n",
       "        [ 0.3571788 ,  0.09537017],\n",
       "        [-1.006651  , -0.8065684 ]], dtype=float32)>,\n",
       " <tf.Variable 'dense_1/bias:0' shape=(2,) dtype=float32, numpy=array([0., 0.], dtype=float32)>]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "layer.trainable_variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dense_1/kernel:0, shape: (3, 2)\n",
      "<tf.Variable 'dense_1/kernel:0' shape=(3, 2) dtype=float32, numpy=\n",
      "array([[-1.0683123 ,  0.6282842 ],\n",
      "       [ 0.3571788 ,  0.09537017],\n",
      "       [-1.006651  , -0.8065684 ]], dtype=float32)> [<tf.Tensor: shape=(3, 2), dtype=float32, numpy=\n",
      "array([[0., 0.],\n",
      "       [0., 0.],\n",
      "       [0., 0.]], dtype=float32)>, <tf.Tensor: shape=(2,), dtype=float32, numpy=array([0., 0.], dtype=float32)>]\n",
      "dense_1/bias:0, shape: (2,)\n",
      "<tf.Variable 'dense_1/bias:0' shape=(2,) dtype=float32, numpy=array([0., 0.], dtype=float32)> [<tf.Tensor: shape=(3, 2), dtype=float32, numpy=\n",
      "array([[0., 0.],\n",
      "       [0., 0.],\n",
      "       [0., 0.]], dtype=float32)>, <tf.Tensor: shape=(2,), dtype=float32, numpy=array([0., 0.], dtype=float32)>]\n"
     ]
    }
   ],
   "source": [
    "for var,g in zip( layer.trainable_variables, grad):\n",
    "    print(f'{var.name}, shape: {g.shape}')\n",
    "    print(var, grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "f6Gx6LS714zR"
   },
   "source": [
    "<a id=\"watches\"></a>\n",
    "\n",
    "## Controlling what the tape watches"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "N4VlqKFzzGaC"
   },
   "source": [
    "The default behavior is to record all operations after accessing a trainable `tf.Variable`. The reasons for this are:\n",
    "\n",
    "* The tape needs to know which operations to record in the forward pass to calculate the gradients in the backwards pass.\n",
    "* The tape holds references to intermediate outputs, so you don't want to record unnecessary operations.\n",
    "* The most common use case involves calculating the gradient of a loss with respect to all a model's trainable variables.\n",
    "\n",
    "For example, the following fails to calculate a gradient because the `tf.Tensor` is not \"watched\" by default, and the `tf.Variable` is not trainable:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "Kj9gPckdB37a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(6.0, shape=(), dtype=float32)\n",
      "None\n",
      "None\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# A trainable variable\n",
    "x0 = tf.Variable(3.0, name='x0')\n",
    "# Not trainable\n",
    "x1 = tf.Variable(3.0, name='x1', trainable=False)\n",
    "# Not a Variable: A variable + tensor returns a tensor.\n",
    "x2 = tf.Variable(2.0, name='x2') + 1.0\n",
    "# Not a variable\n",
    "x3 = tf.constant(3.0, name='x3')\n",
    "\n",
    "with tf.GradientTape() as tape:\n",
    "  y = (x0**2) + (x1**2) + (x2**2)\n",
    "\n",
    "grad = tape.gradient(y, [x0, x1, x2, x3])\n",
    "\n",
    "for g in grad:\n",
    "  print(g)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RkcpQnLgNxgi"
   },
   "source": [
    "You can list the variables being watched by the tape using the `GradientTape.watched_variables` method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "hwNwjW1eAkib"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['x0:0']"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[var.name for var in tape.watched_variables()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NB9I1uFvB4tf"
   },
   "source": [
    "`tf.GradientTape` provides hooks that give the user control over what is or is not watched.\n",
    "\n",
    "To record gradients with respect to a `tf.Tensor`, you need to call `GradientTape.watch(x)`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "id": "tVN1QqFRDHBK"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6.0\n"
     ]
    }
   ],
   "source": [
    "x = tf.constant(3.0)\n",
    "with tf.GradientTape() as tape:\n",
    "  tape.watch(x)\n",
    "  y = x**2\n",
    "\n",
    "# dy = 2x * dx\n",
    "dy_dx = tape.gradient(y, x)\n",
    "print(dy_dx.numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qxsiYnf2DN8K"
   },
   "source": [
    "Conversely, to disable the default behavior of watching all `tf.Variables`, set `watch_accessed_variables=False` when creating the gradient tape. This calculation uses two variables, but only connects the gradient for one of the variables:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "id": "7QPzwWvSEwIp"
   },
   "outputs": [],
   "source": [
    "x0 = tf.Variable(0.0)\n",
    "x1 = tf.Variable(10.0)\n",
    "\n",
    "with tf.GradientTape(watch_accessed_variables=False) as tape:\n",
    "  tape.watch(x1)\n",
    "  y0 = tf.math.sin(x0)\n",
    "  y1 = tf.nn.softplus(x1)\n",
    "  y = y0 + y1\n",
    "  ys = tf.reduce_sum(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TRduLbE1H2IJ"
   },
   "source": [
    "Since `GradientTape.watch` was not called on `x0`, no gradient is computed with respect to it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "id": "e6GM-3evH1Sz"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dy/dx0: None\n",
      "dy/dx1: 0.9999546\n"
     ]
    }
   ],
   "source": [
    "# dys/dx1 = exp(x1) / (1 + exp(x1)) = sigmoid(x1)\n",
    "grad = tape.gradient(ys, {'x0': x0, 'x1': x1})\n",
    "\n",
    "print('dy/dx0:', grad['x0'])\n",
    "print('dy/dx1:', grad['x1'].numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2g1nKB6P-OnA"
   },
   "source": [
    "## Intermediate results\n",
    "\n",
    "You can also request gradients of the output with respect to intermediate values computed inside the `tf.GradientTape` context."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "id": "7XaPRAwUyYms"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18.0\n"
     ]
    }
   ],
   "source": [
    "x = tf.constant(3.0)\n",
    "\n",
    "with tf.GradientTape() as tape:\n",
    "  tape.watch(x)\n",
    "  y = x * x\n",
    "  z = y * y\n",
    "\n",
    "# Use the tape to compute the gradient of z with respect to the\n",
    "# intermediate value y.\n",
    "# dz_dy = 2 * y and y = x ** 2 = 9\n",
    "print(tape.gradient(z, y).numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ISkXuY7YzIcS"
   },
   "source": [
    "By default, the resources held by a `GradientTape` are released as soon as the `GradientTape.gradient` method is called. To compute multiple gradients over the same computation, create a gradient tape with `persistent=True`. This allows multiple calls to the `gradient` method as resources are released when the tape object is garbage collected. For example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "id": "zZaCm3-9zVCi"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[  4. 108.]\n",
      "[2. 6.]\n"
     ]
    }
   ],
   "source": [
    "x = tf.constant([1, 3.0])\n",
    "with tf.GradientTape(persistent=True) as tape:\n",
    "  tape.watch(x)\n",
    "  y = x * x\n",
    "  z = y * y\n",
    "\n",
    "print(tape.gradient(z, x).numpy())  # 108.0 (4 * x**3 at x = 3)\n",
    "print(tape.gradient(y, x).numpy())  # 6.0 (2 * x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "id": "j8bv_jQFg6CN"
   },
   "outputs": [],
   "source": [
    "del tape   # Drop the reference to the tape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "O_ZY-9BUB7vX"
   },
   "source": [
    "## Notes on performance\n",
    "\n",
    "* There is a tiny overhead associated with doing operations inside a gradient tape context. For most eager execution this will not be a noticeable cost, but you should still use tape context around the areas only where it is required.\n",
    "\n",
    "* Gradient tapes use memory to store intermediate results, including inputs and outputs, for use during the backwards pass.\n",
    "\n",
    "  For efficiency, some ops (like `ReLU`) don't need to keep their intermediate results and they are pruned during the forward pass. However, if you use `persistent=True` on your tape, *nothing is discarded* and your peak memory usage will be higher."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9dLBpZsJebFq"
   },
   "source": [
    "## Gradients of non-scalar targets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7pldU9F5duP2"
   },
   "source": [
    "A gradient is fundamentally an operation on a scalar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "id": "qI0sDV_WeXBb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.0\n",
      "-0.25\n"
     ]
    }
   ],
   "source": [
    "x = tf.Variable(2.0)\n",
    "with tf.GradientTape(persistent=True) as tape:\n",
    "  y0 = x**2\n",
    "  y1 = 1 / x\n",
    "\n",
    "print(tape.gradient(y0, x).numpy())\n",
    "print(tape.gradient(y1, x).numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "COEyYp34fxj4"
   },
   "source": [
    "Thus, if you ask for the gradient of multiple targets, the result for each source is:\n",
    "\n",
    "* The gradient of the sum of the targets, or equivalently\n",
    "* The sum of the gradients of each target."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "id": "o4a6_YOcfWKS"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.75\n"
     ]
    }
   ],
   "source": [
    "x = tf.Variable(2.0)\n",
    "with tf.GradientTape() as tape:\n",
    "  y0 = x**2\n",
    "  y1 = 1 / x\n",
    "\n",
    "print(tape.gradient({'y0': y0, 'y1': y1}, x).numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uvP-mkBMgbym"
   },
   "source": [
    "Similarly, if the target(s) are not scalar the gradient of the sum is calculated:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "id": "DArPWqsSh5un"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7.0\n"
     ]
    }
   ],
   "source": [
    "x = tf.Variable(2.)\n",
    "\n",
    "with tf.GradientTape() as tape:\n",
    "  y = x * [3., 4.]\n",
    "\n",
    "print(tape.gradient(y, x).numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "flDbx68Zh5Lb"
   },
   "source": [
    "This makes it simple to take the gradient of the sum of a collection of losses, or the gradient of the sum of an element-wise loss calculation.\n",
    "\n",
    "If you need a separate gradient for each item, refer to [Jacobians](advanced_autodiff.ipynb#jacobians)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iwFswok8RAly"
   },
   "source": [
    "In some cases you can skip the Jacobian. For an element-wise calculation, the gradient of the sum gives the derivative of each element with respect to its input-element, since each element is independent:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = tf.linspace(-10., 10., 1000+1)\n",
    "\n",
    "with tf.GradientTape() as tape:\n",
    "    tape.watch(x)\n",
    "    y = tf.nn.sigmoid(x)\n",
    "\n",
    "dy_dx = tape.gradient(y, x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0.5, 0, 'x')"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(x, y, label='y')\n",
    "plt.plot(x, dy_dx, label='dy/dx')\n",
    "plt.legend(loc='center right')\n",
    "plt.xlabel('x')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6kADybtQzYj4"
   },
   "source": [
    "## Control flow\n",
    "\n",
    "Because a gradient tape records operations as they are executed, Python control flow is naturally handled (for example, `if` and `while` statements).\n",
    "\n",
    "Here a different variable is used on each branch of an `if`. The gradient only connects to the variable that was used:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "id": "ciFLizhrrjy7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(1.0, shape=(), dtype=float32)\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "x = tf.constant(1.0)\n",
    "# x = tf.constant(-1.0)\n",
    "\n",
    "v0 = tf.Variable(2.0)\n",
    "v1 = tf.Variable(20.0)\n",
    "\n",
    "with tf.GradientTape(persistent=True) as tape:\n",
    "  tape.watch(x)\n",
    "  if x > 0.0:\n",
    "    result = v0\n",
    "  else:\n",
    "    result = v1**2 \n",
    "\n",
    "dv0, dv1 = tape.gradient(result, [v0, v1])\n",
    "\n",
    "print(dv0)\n",
    "print(dv1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HKnLaiapsjeP"
   },
   "source": [
    "Just remember that the control statements themselves are not differentiable, so they are invisible to gradient-based optimizers.\n",
    "\n",
    "Depending on the value of `x` in the above example, the tape either records `result = v0` or `result = v1**2`. The gradient with respect to `x` is always `None`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "id": "8k05WmuAwPm7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    }
   ],
   "source": [
    "dx = tape.gradient(result, x)\n",
    "\n",
    "print(dx)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "egypBxISAHhx"
   },
   "source": [
    "## Getting a gradient of `None`\n",
    "\n",
    "When a target is not connected to a source you will get a gradient of `None`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "id": "CU185WDM81Ut"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    }
   ],
   "source": [
    "x = tf.Variable(2.)\n",
    "y = tf.Variable(3.)\n",
    "\n",
    "with tf.GradientTape() as tape:\n",
    "  z = y * y\n",
    "print(tape.gradient(z, x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sZbKpHfBRJym"
   },
   "source": [
    "Here `z` is obviously not connected to `x`, but there are several less-obvious ways that a gradient can be disconnected."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eHDzDOiQ8xmw"
   },
   "source": [
    "### 1. Replaced a variable with a tensor\n",
    "\n",
    "In the section on [\"controlling what the tape watches\"](#watches) you saw that the tape will automatically watch a `tf.Variable` but not a `tf.Tensor`.\n",
    "\n",
    "One common error is to inadvertently replace a `tf.Variable` with a `tf.Tensor`, instead of using `Variable.assign` to update the `tf.Variable`. Here is an example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "id": "QPKY4Tn9zX7_"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ResourceVariable : tf.Tensor(1.0, shape=(), dtype=float32)\n",
      "EagerTensor : None\n"
     ]
    }
   ],
   "source": [
    "x = tf.Variable(2.0)\n",
    "\n",
    "for epoch in range(2):\n",
    "  with tf.GradientTape() as tape:\n",
    "    y = x+1\n",
    "\n",
    "  print(type(x).__name__, \":\", tape.gradient(y, x))\n",
    "  x = x + 1   # This should be `x.assign_add(1)`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3gwZKxgA97an"
   },
   "source": [
    "### 2. Did calculations outside of TensorFlow\n",
    "\n",
    "The tape can't record the gradient path if the calculation exits TensorFlow.\n",
    "For example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "id": "jmoLCDJb_yw1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    }
   ],
   "source": [
    "x = tf.Variable([[1.0, 2.0],\n",
    "                 [3.0, 4.0]], dtype=tf.float32)\n",
    "\n",
    "with tf.GradientTape() as tape:\n",
    "  x2 = x**2\n",
    "\n",
    "  # This step is calculated with NumPy\n",
    "  y = np.mean(x2, axis=0)\n",
    "\n",
    "  # Like most ops, reduce_mean will cast the NumPy array to a constant tensor\n",
    "  # using `tf.convert_to_tensor`.\n",
    "  y = tf.reduce_mean(y, axis=0)\n",
    "\n",
    "print(tape.gradient(y, x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "p3YVfP3R-tp7"
   },
   "source": [
    "### 3. Took gradients through an integer or string\n",
    "\n",
    "Integers and strings are not differentiable. If a calculation path uses these data types there will be no gradient.\n",
    "\n",
    "Nobody expects strings to be differentiable, but it's easy to accidentally create an `int` constant or variable if you don't specify the `dtype`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "id": "9jlHXHqfASU3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:The dtype of the watched tensor must be floating (e.g. tf.float32), got tf.int32\n",
      "WARNING:tensorflow:The dtype of the target tensor must be floating (e.g. tf.float32) when calling GradientTape.gradient, got tf.int32\n",
      "WARNING:tensorflow:The dtype of the source tensor must be floating (e.g. tf.float32) when calling GradientTape.gradient, got tf.int32\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "x = tf.constant(10)\n",
    "\n",
    "with tf.GradientTape() as g:\n",
    "  g.watch(x)\n",
    "  y = x * x\n",
    "\n",
    "print(g.gradient(y, x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RsdP_mTHX9L1"
   },
   "source": [
    "TensorFlow doesn't automatically cast between types, so, in practice, you'll often get a type error instead of a missing gradient."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WyAZ7C8qCEs6"
   },
   "source": [
    "### 4. Took gradients through a stateful object\n",
    "\n",
    "State stops gradients. When you read from a stateful object, the tape can only observe the current state, not the history that lead to it.\n",
    "\n",
    "A `tf.Tensor` is immutable. You can't change a tensor once it's created. It has a _value_, but no _state_. All the operations discussed so far are also stateless: the output of a `tf.matmul` only depends on its inputs.\n",
    "\n",
    "A `tf.Variable` has internal stateits value. When you use the variable, the state is read. It's normal to calculate a gradient with respect to a variable, but the variable's state blocks gradient calculations from going farther back. For example:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "id": "C1tLeeRFE479"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    }
   ],
   "source": [
    "x0 = tf.Variable(3.0)\n",
    "x1 = tf.Variable(0.0)\n",
    "\n",
    "with tf.GradientTape() as tape:\n",
    "  # Update x1 = x1 + x0.\n",
    "  x1.assign_add(x0)\n",
    "  # The tape starts recording from x1.\n",
    "  y = x1**2   # y = (x1 + x0)**2\n",
    "\n",
    "# This doesn't work.\n",
    "print(tape.gradient(y, x0))   #dy/dx0 = 2*(x1 + x0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xKA92-dqF2r-"
   },
   "source": [
    "Similarly, `tf.data.Dataset` iterators and `tf.queue`s are stateful, and will stop all gradients on tensors that pass through them."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HHvcDGIbOj2I"
   },
   "source": [
    "## No gradient registered"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aoc-A6AxVqry"
   },
   "source": [
    "Some `tf.Operation`s are **registered as being non-differentiable** and will return `None`. Others have **no gradient registered**.\n",
    "\n",
    "The `tf.raw_ops` page shows which low-level ops have gradients registered.\n",
    "\n",
    "If you attempt to take a gradient through a float op that has no gradient registered the tape will throw an error instead of silently returning `None`. This way you know something has gone wrong.\n",
    "\n",
    "For example, the `tf.image.adjust_contrast` function wraps `raw_ops.AdjustContrastv2`, which could have a gradient but the gradient is not implemented:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "id": "HSb20FXc_V0U"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LookupError: gradient registry has no entry for: AdjustContrastv2\n"
     ]
    }
   ],
   "source": [
    "image = tf.Variable([[[0.5, 0.0, 0.0]]])\n",
    "delta = tf.Variable(0.1)\n",
    "\n",
    "with tf.GradientTape() as tape:\n",
    "  new_image = tf.image.adjust_contrast(image, delta)\n",
    "\n",
    "try:\n",
    "  print(tape.gradient(new_image, [image, delta]))\n",
    "  assert False   # This should not happen.\n",
    "except LookupError as e:\n",
    "  print(f'{type(e).__name__}: {e}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pDoutjzATiEm"
   },
   "source": [
    "If you need to differentiate through this op, you'll either need to implement the gradient and register it (using `tf.RegisterGradient`) or re-implement the function using other ops."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GCTwc_dQXp2W"
   },
   "source": [
    "## Zeros instead of None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TYDrVogA89eA"
   },
   "source": [
    "In some cases it would be convenient to get 0 instead of `None` for unconnected gradients.  You can decide what to return when you have unconnected gradients using the `unconnected_gradients` argument:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "id": "U6zxk1sf9Ixx"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor([0. 0.], shape=(2,), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "x = tf.Variable([2., 2.])\n",
    "y = tf.Variable(3.)\n",
    "\n",
    "with tf.GradientTape() as tape:\n",
    "  z = y**2\n",
    "print(tape.gradient(z, x, unconnected_gradients=tf.UnconnectedGradients.ZERO))"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "Tce3stUlHN0L"
   ],
   "name": "autodiff.ipynb",
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
