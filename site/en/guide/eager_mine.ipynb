{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CCQY7jpBfMur"
   },
   "source": [
    "##### Copyright 2018 The TensorFlow Authors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "cellView": "form",
    "id": "z6X9omPnfO_h"
   },
   "outputs": [],
   "source": [
    "#@title Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "# https://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2QQJJyDzqGRb"
   },
   "source": [
    "# Eager execution\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "B1xdylywqUSX"
   },
   "source": [
    "<table class=\"tfo-notebook-buttons\" align=\"left\">\n",
    "  <td>\n",
    "    <a target=\"_blank\" href=\"https://www.tensorflow.org/guide/eager\"><img src=\"https://www.tensorflow.org/images/tf_logo_32px.png\" />View on TensorFlow.org</a>\n",
    "  </td>\n",
    "  <td>\n",
    "    <a target=\"_blank\" href=\"https://colab.research.google.com/github/tensorflow/docs/blob/master/site/en/guide/eager.ipynb\"><img src=\"https://www.tensorflow.org/images/colab_logo_32px.png\" />Run in Google Colab</a>\n",
    "  </td>\n",
    "  <td>\n",
    "    <a target=\"_blank\" href=\"https://github.com/tensorflow/docs/blob/master/site/en/guide/eager.ipynb\"><img src=\"https://www.tensorflow.org/images/GitHub-Mark-32px.png\" />View source on GitHub</a>\n",
    "  </td>\n",
    "  <td>\n",
    "    <a href=\"https://storage.googleapis.com/tensorflow_docs/docs/site/en/guide/eager.ipynb\"><img src=\"https://www.tensorflow.org/images/download_logo_32px.png\" />Download notebook</a>\n",
    "  </td>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EGjDcGxIqEfX"
   },
   "source": [
    "TensorFlow's eager execution is an imperative programming environment that\n",
    "evaluates operations immediately, without building graphs: operations return\n",
    "concrete values instead of constructing a computational graph to run later. This\n",
    "makes it easy to get started with TensorFlow and debug models, and it\n",
    "reduces boilerplate as well. To follow along with this guide, run the code\n",
    "samples below in an interactive `python` interpreter.\n",
    "\n",
    "Eager execution is a flexible machine learning platform for research and\n",
    "experimentation, providing:\n",
    "\n",
    "* *An intuitive interface*—Structure your code naturally and use Python data\n",
    "  structures. Quickly iterate on small models and small data.\n",
    "* *Easier debugging*—Call ops directly to inspect running models and test\n",
    "  changes. Use standard Python debugging tools for immediate error reporting.\n",
    "* *Natural control flow*—Use Python control flow instead of graph control\n",
    "  flow, simplifying the specification of dynamic models.\n",
    "\n",
    "Eager execution supports most TensorFlow operations and GPU acceleration.\n",
    "\n",
    "Note: Some models may experience increased overhead with eager execution\n",
    "enabled. Performance improvements are ongoing, but please\n",
    "[file a bug](https://github.com/tensorflow/tensorflow/issues) if you find a\n",
    "problem and share your benchmarks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RBAeIwOMrYk8"
   },
   "source": [
    "## Setup and basic usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "id": "ByNsp4VqqEfa"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "import cProfile\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "48P3-8q4qEfe"
   },
   "source": [
    "In Tensorflow 2.0, eager execution is enabled by default."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Try running this notebook with Eager disabled and once with Eager enabled.\n",
    "    Note - the below numpy code does not work without Eager as in disabled \n",
    "    moode, tf tensor can't be passed to numpy.\n",
    "'''\n",
    "if 0:\n",
    "    tf.compat.v1.disable_eager_execution()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[2.0]]"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = [[2.0]]\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor([[4.]], shape=(1, 1), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "m = tf.matmul(x, x)\n",
    "print(m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "id": "7aFsD8csqEff"
   },
   "outputs": [],
   "source": [
    "# this is the default mode. Need not run this.\n",
    "# tf.executing_eagerly()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "x_G1zZT5qEfh"
   },
   "source": [
    "Now you can run TensorFlow operations and the results will return immediately:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "id": "9gsI54pbqEfj"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hello, [[4.]]\n"
     ]
    }
   ],
   "source": [
    "x = [[2.]]\n",
    "m = tf.matmul(x, x)\n",
    "print(\"hello, {}\".format(m))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ajFn6qsdqEfl"
   },
   "source": [
    "Enabling eager execution changes how TensorFlow operations behave—now they\n",
    "immediately evaluate and return their values to Python. `tf.Tensor` objects\n",
    "reference concrete values instead of symbolic handles to nodes in a computational\n",
    "graph. Since there isn't a computational graph to build and run later in a\n",
    "session, it's easy to inspect results using `print()` or a debugger. Evaluating,\n",
    "printing, and checking tensor values does not break the flow for computing\n",
    "gradients.\n",
    "\n",
    "Eager execution works nicely with [NumPy](http://www.numpy.org/). NumPy\n",
    "operations accept `tf.Tensor` arguments. The TensorFlow\n",
    "`tf.math` operations convert\n",
    "Python objects and NumPy arrays to `tf.Tensor` objects. The\n",
    "`tf.Tensor.numpy` method returns the object's value as a NumPy `ndarray`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "id": "sTO0_5TYqz1n"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[1 2]\n",
      " [3 4]], shape=(2, 2), dtype=int32)\n"
     ]
    }
   ],
   "source": [
    "a = tf.constant([ [1,2],\n",
    "                  [3,4] ])\n",
    "print(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "id": "Dp14YT8Gq4r1"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(2, 2), dtype=int32, numpy=\n",
       "array([[2, 3],\n",
       "       [4, 5]], dtype=int32)>"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Broadcasting support\n",
    "b = tf.add(a, 1)\n",
    "b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "id": "69p3waMfq8cQ"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[ 2  6]\n",
      " [12 20]], shape=(2, 2), dtype=int32)\n"
     ]
    }
   ],
   "source": [
    "# Operator overloading is supported\n",
    "print(a * b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "id": "Ui025t1qqEfm"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 2,  6],\n",
       "       [12, 20]], dtype=int32)"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Use NumPy values\n",
    "import numpy as np\n",
    "\n",
    "c = np.multiply(a, b)\n",
    "c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "id": "Tq_aFRzWrCua"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1 2]\n",
      " [3 4]]\n"
     ]
    }
   ],
   "source": [
    "# Obtain numpy value from a tensor:\n",
    "print(a.numpy())\n",
    "# => [[1 2]\n",
    "#     [3 4]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "H08f9ss9qEft"
   },
   "source": [
    "## Dynamic control flow\n",
    "\n",
    "A major benefit of eager execution is that all the functionality of the host\n",
    "language is available while your model is executing. So, for example,\n",
    "it is easy to write [fizzbuzz](https://en.wikipedia.org/wiki/Fizz_buzz):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fb(num):\n",
    "    idx = tf.constant(0)\n",
    "    num = tf.convert_to_tensor(num)\n",
    "    for n in range(num):\n",
    "        n = tf.constant(n+1)\n",
    "        if n%3 == 0 and n%5 == 0:\n",
    "            print(\"fb\")\n",
    "        elif n%3 == 0:\n",
    "            print('f')\n",
    "        elif n%5 == 0:\n",
    "            print('b')\n",
    "        else:\n",
    "            print(n.numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "2\n",
      "f\n",
      "4\n",
      "b\n",
      "f\n",
      "7\n",
      "8\n",
      "f\n",
      "b\n",
      "11\n",
      "f\n",
      "13\n",
      "14\n",
      "fb\n"
     ]
    }
   ],
   "source": [
    "fb(15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7kA-aC3BqEfy"
   },
   "source": [
    "This has conditionals that depend on tensor values and it prints these values\n",
    "at runtime."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8huKpuuAwICq"
   },
   "source": [
    "## Eager training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mp2lCCZYrxHd"
   },
   "source": [
    "### Computing gradients\n",
    "\n",
    "[Automatic differentiation](https://en.wikipedia.org/wiki/Automatic_differentiation)\n",
    "is useful for implementing machine learning algorithms such as\n",
    "[backpropagation](https://en.wikipedia.org/wiki/Backpropagation) for training\n",
    "neural networks. During eager execution, use `tf.GradientTape` to trace\n",
    "operations for computing gradients later.\n",
    "\n",
    "You can use `tf.GradientTape` to train and/or compute gradients in eager. It is especially useful for complicated training loops.  \n",
    "\n",
    "Since different operations can occur during each call, all\n",
    "forward-pass operations get recorded to a \"tape\". To compute the gradient, play\n",
    "the tape backwards and then discard. A particular `tf.GradientTape` can only\n",
    "compute one gradient; subsequent calls throw a runtime error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([12.], dtype=float32)"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w = tf.Variable([2.])\n",
    "with tf.GradientTape() as tape:\n",
    "    loss = w*w*w\n",
    "    \n",
    "grad = tape.gradient(loss, w)\n",
    "grad.numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vkHs32GqweYS"
   },
   "source": [
    "### Train a model\n",
    "\n",
    "The following example creates a multi-layer model that classifies the standard\n",
    "MNIST handwritten digits. It demonstrates the optimizer and layer APIs to build\n",
    "trainable graphs in an eager execution environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "# try keeping batch_size as a multiple of 60k, else the assert will fail for the last batch having partial size\n",
    "batch_size = 60\n",
    "num_epochs = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60000, 28, 28)\n"
     ]
    }
   ],
   "source": [
    "# Fetch and format the mnist data\n",
    "(mnist_imgs, mnist_labels), _ = tf.keras.datasets.mnist.load_data()\n",
    "print(mnist_imgs.shape)\n",
    "\n",
    "dataset = tf.data.Dataset.from_tensor_slices(\n",
    "(tf.cast(mnist_imgs[..., tf.newaxis]/255., tf.float32)\n",
    ", tf.cast(mnist_labels, tf.int64)))\n",
    "\n",
    "dataset = dataset.shuffle(1024).batch(batch_size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60000, 28, 28)\n",
      "(60000, 28, 28, 1)\n"
     ]
    }
   ],
   "source": [
    "print(mnist_imgs[...].shape)\n",
    "print(mnist_imgs[..., tf.newaxis].shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build the model\n",
    "mnist_model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Conv2D(16, [3,3], activation='relu', input_shape=[None, None, 1])\n",
    "    , tf.keras.layers.Conv2D(16, [3,3], activation='relu')\n",
    "    , tf.keras.layers.GlobalAveragePooling2D()\n",
    "    , tf.keras.layers.Dense(10)\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fvyk-HgGwxwl"
   },
   "source": [
    "Even without training, call the model and inspect the output in eager execution:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "60\n",
      "Logits:  [[ 0.04959699 -0.01434247 -0.02356266  0.03411499  0.02188966  0.02197097\n",
      "  -0.05428361 -0.0210592   0.03953082  0.01866637]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<TakeDataset shapes: ((None, 28, 28, 1), (None,)), types: (tf.float32, tf.int64)>"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# fetch one batch and  run forward pass on one image of that batch\n",
    "for imgs, labels in dataset.take(1):\n",
    "    print(len(imgs))\n",
    "    print(\"Logits: \", mnist_model(imgs[0:1]).numpy() )\n",
    "dataset.take(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Y3PGa8G7qEgB"
   },
   "source": [
    "While keras models have a builtin training loop (using the `fit` method), sometimes you need more customization. Here's an example, of a training loop implemented with eager:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tXaupYXRI2YM"
   },
   "source": [
    "Note: Use the assert functions in `tf.debugging` to check if a condition holds up. This works in eager and graph execution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = tf.keras.optimizers.Adam()\n",
    "loss_obj = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
    "loss_hist = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_step(imgs, labels):\n",
    "    with tf.GradientTape() as tape:\n",
    "        logits = mnist_model(imgs, training=True)\n",
    "        \n",
    "        # Add asserts to check the shape of the output.\n",
    "        tf.debugging.assert_equal(logits.shape, (batch_size,10))\n",
    "        \n",
    "        loss_val = loss_obj(labels, logits)\n",
    "                \n",
    "    loss_hist.append(loss_val.numpy().mean())\n",
    "    grads = tape.gradient(loss_val, mnist_model.trainable_variables)\n",
    "    optimizer.apply_gradients( zip(grads, mnist_model.trainable_variables) )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(epochs):\n",
    "    for e in range(epochs):\n",
    "        for (batch, (imgs, labels)) in enumerate(dataset):\n",
    "            train_step(imgs,labels)\n",
    "        print('Epoch {} Ends '.format(e))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 Ends \n"
     ]
    }
   ],
   "source": [
    "train(epochs=num_epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[2.3036144,\n",
       " 2.3023791,\n",
       " 2.2994573,\n",
       " 2.305358,\n",
       " 2.3022652,\n",
       " 2.3079572,\n",
       " 2.3060129,\n",
       " 2.294482,\n",
       " 2.3009706,\n",
       " 2.300827,\n",
       " 2.297202,\n",
       " 2.3013062,\n",
       " 2.2967196,\n",
       " 2.307391,\n",
       " 2.3073668,\n",
       " 2.2923594,\n",
       " 2.30245,\n",
       " 2.295536,\n",
       " 2.298321,\n",
       " 2.2882202,\n",
       " 2.2978683,\n",
       " 2.3034074,\n",
       " 2.3023188,\n",
       " 2.2975383,\n",
       " 2.2884212,\n",
       " 2.291364,\n",
       " 2.3034043,\n",
       " 2.3039043,\n",
       " 2.2938235,\n",
       " 2.2909954,\n",
       " 2.3130875,\n",
       " 2.294762,\n",
       " 2.3090022,\n",
       " 2.3015249,\n",
       " 2.2908845,\n",
       " 2.3039389,\n",
       " 2.3084848,\n",
       " 2.293018,\n",
       " 2.292021,\n",
       " 2.2997792,\n",
       " 2.288609,\n",
       " 2.300794,\n",
       " 2.295703,\n",
       " 2.3053825,\n",
       " 2.284878,\n",
       " 2.2824771,\n",
       " 2.2861862,\n",
       " 2.298138,\n",
       " 2.320782,\n",
       " 2.2991529,\n",
       " 2.300612,\n",
       " 2.3005898,\n",
       " 2.2881424,\n",
       " 2.2785823,\n",
       " 2.3056848,\n",
       " 2.273457,\n",
       " 2.2922719,\n",
       " 2.2859743,\n",
       " 2.2991698,\n",
       " 2.2928925,\n",
       " 2.2939696,\n",
       " 2.3016348,\n",
       " 2.2829757,\n",
       " 2.3004317,\n",
       " 2.295487,\n",
       " 2.2793367,\n",
       " 2.2845626,\n",
       " 2.29319,\n",
       " 2.2824178,\n",
       " 2.28421,\n",
       " 2.286727,\n",
       " 2.2836156,\n",
       " 2.2722032,\n",
       " 2.2840464,\n",
       " 2.2737172,\n",
       " 2.3055804,\n",
       " 2.2648232,\n",
       " 2.2670934,\n",
       " 2.2813673,\n",
       " 2.270993,\n",
       " 2.3062665,\n",
       " 2.2836766,\n",
       " 2.2897446,\n",
       " 2.274052,\n",
       " 2.2537532,\n",
       " 2.2934728,\n",
       " 2.2713602,\n",
       " 2.3049133,\n",
       " 2.288794,\n",
       " 2.2712464,\n",
       " 2.2757766,\n",
       " 2.2460744,\n",
       " 2.2705536,\n",
       " 2.278659,\n",
       " 2.2818186,\n",
       " 2.263808,\n",
       " 2.281403,\n",
       " 2.2659886,\n",
       " 2.2606237,\n",
       " 2.277759,\n",
       " 2.26019,\n",
       " 2.2910044,\n",
       " 2.2880702,\n",
       " 2.3011794,\n",
       " 2.2740843,\n",
       " 2.2920384,\n",
       " 2.291713,\n",
       " 2.2907526,\n",
       " 2.2424016,\n",
       " 2.2722843,\n",
       " 2.2873743,\n",
       " 2.2536387,\n",
       " 2.2468593,\n",
       " 2.260171,\n",
       " 2.2476363,\n",
       " 2.2870255,\n",
       " 2.286157,\n",
       " 2.2626262,\n",
       " 2.284139,\n",
       " 2.2556596,\n",
       " 2.2767024,\n",
       " 2.2638178,\n",
       " 2.2284856,\n",
       " 2.2661738,\n",
       " 2.249381,\n",
       " 2.2578783,\n",
       " 2.245635,\n",
       " 2.2304692,\n",
       " 2.2495296,\n",
       " 2.2774568,\n",
       " 2.2514522,\n",
       " 2.2346714,\n",
       " 2.2600853,\n",
       " 2.2159247,\n",
       " 2.2799444,\n",
       " 2.2470794,\n",
       " 2.2461927,\n",
       " 2.2261662,\n",
       " 2.2392614,\n",
       " 2.2730184,\n",
       " 2.2574716,\n",
       " 2.2752836,\n",
       " 2.2421448,\n",
       " 2.2087352,\n",
       " 2.2872577,\n",
       " 2.2505672,\n",
       " 2.2240908,\n",
       " 2.1800535,\n",
       " 2.2496946,\n",
       " 2.2246327,\n",
       " 2.1718984,\n",
       " 2.239758,\n",
       " 2.2185485,\n",
       " 2.2512157,\n",
       " 2.227787,\n",
       " 2.216797,\n",
       " 2.2391918,\n",
       " 2.2208252,\n",
       " 2.2405822,\n",
       " 2.1961222,\n",
       " 2.217098,\n",
       " 2.2048807,\n",
       " 2.1914358,\n",
       " 2.2172918,\n",
       " 2.2448552,\n",
       " 2.2055979,\n",
       " 2.1693995,\n",
       " 2.2474065,\n",
       " 2.231563,\n",
       " 2.1814241,\n",
       " 2.190876,\n",
       " 2.1667998,\n",
       " 2.208505,\n",
       " 2.2114632,\n",
       " 2.1796854,\n",
       " 2.1729467,\n",
       " 2.2082412,\n",
       " 2.1881256,\n",
       " 2.1916344,\n",
       " 2.1756349,\n",
       " 2.196144,\n",
       " 2.1317167,\n",
       " 2.1975636,\n",
       " 2.1475282,\n",
       " 2.1970727,\n",
       " 2.206903,\n",
       " 2.2266247,\n",
       " 2.1671367,\n",
       " 2.1909277,\n",
       " 2.1908417,\n",
       " 2.1623347,\n",
       " 2.1935623,\n",
       " 2.1593812,\n",
       " 2.1777117,\n",
       " 2.157079,\n",
       " 2.1501045,\n",
       " 2.1636233,\n",
       " 2.1469781,\n",
       " 2.1387732,\n",
       " 2.2202544,\n",
       " 2.2283168,\n",
       " 2.2070181,\n",
       " 2.1644754,\n",
       " 2.2247024,\n",
       " 2.1148279,\n",
       " 2.1674259,\n",
       " 2.1888552,\n",
       " 2.2115371,\n",
       " 2.1077085,\n",
       " 2.1534655,\n",
       " 2.1415052,\n",
       " 2.1568782,\n",
       " 2.1338186,\n",
       " 2.1980731,\n",
       " 2.140917,\n",
       " 2.158315,\n",
       " 2.1622164,\n",
       " 2.1518936,\n",
       " 2.1247742,\n",
       " 2.1710815,\n",
       " 2.1708963,\n",
       " 2.233047,\n",
       " 2.1323934,\n",
       " 2.1497111,\n",
       " 2.1160166,\n",
       " 2.2034721,\n",
       " 2.1729589,\n",
       " 2.14428,\n",
       " 2.2223513,\n",
       " 2.1538844,\n",
       " 2.0943873,\n",
       " 2.2054636,\n",
       " 2.0647728,\n",
       " 2.1824107,\n",
       " 2.1695824,\n",
       " 2.1662786,\n",
       " 2.1395454,\n",
       " 2.1089036,\n",
       " 2.1477625,\n",
       " 2.071689,\n",
       " 2.0435894,\n",
       " 2.0959744,\n",
       " 2.0816915,\n",
       " 2.175498,\n",
       " 2.0883477,\n",
       " 2.14419,\n",
       " 2.053447,\n",
       " 2.1752594,\n",
       " 2.119215,\n",
       " 2.1035826,\n",
       " 2.0340462,\n",
       " 2.1041043,\n",
       " 2.1220858,\n",
       " 2.0604408,\n",
       " 2.1025205,\n",
       " 2.0830638,\n",
       " 2.119036,\n",
       " 2.1333456,\n",
       " 2.197148,\n",
       " 2.0925443,\n",
       " 2.0045316,\n",
       " 2.0414703,\n",
       " 2.106768,\n",
       " 2.0843174,\n",
       " 2.0991075,\n",
       " 2.1150982,\n",
       " 2.0615547,\n",
       " 2.010039,\n",
       " 1.9760842,\n",
       " 2.1292357,\n",
       " 2.056519,\n",
       " 2.0362232,\n",
       " 2.0096185,\n",
       " 2.170446,\n",
       " 2.0607479,\n",
       " 2.1899116,\n",
       " 2.0848615,\n",
       " 2.1368282,\n",
       " 2.1098788,\n",
       " 2.0675664,\n",
       " 2.065455,\n",
       " 2.1510172,\n",
       " 2.0818129,\n",
       " 2.179099,\n",
       " 2.0355484,\n",
       " 2.0193865,\n",
       " 2.0049994,\n",
       " 1.9680274,\n",
       " 1.9739945,\n",
       " 2.1308405,\n",
       " 2.0648963,\n",
       " 2.1234028,\n",
       " 2.0525975,\n",
       " 2.04731,\n",
       " 2.0872245,\n",
       " 2.0588171,\n",
       " 1.9532874,\n",
       " 2.1200137,\n",
       " 1.9213295,\n",
       " 2.1074824,\n",
       " 2.1041906,\n",
       " 1.9806503,\n",
       " 2.0616496,\n",
       " 2.075021,\n",
       " 2.0959454,\n",
       " 2.1258245,\n",
       " 2.0175672,\n",
       " 1.9952564,\n",
       " 2.1082,\n",
       " 2.0414286,\n",
       " 2.0867617,\n",
       " 2.010784,\n",
       " 2.0756445,\n",
       " 1.9203074,\n",
       " 2.0591009,\n",
       " 2.0854113,\n",
       " 1.9663167,\n",
       " 1.9565346,\n",
       " 2.0066714,\n",
       " 2.0819993,\n",
       " 2.0001721,\n",
       " 2.1515777,\n",
       " 2.0568244,\n",
       " 1.9862792,\n",
       " 2.0260956,\n",
       " 2.053496,\n",
       " 2.0431035,\n",
       " 2.00385,\n",
       " 2.0643034,\n",
       " 2.0456195,\n",
       " 2.0777388,\n",
       " 1.9854819,\n",
       " 1.9689298,\n",
       " 2.0676343,\n",
       " 1.9971433,\n",
       " 2.050143,\n",
       " 1.9756261,\n",
       " 2.0261793,\n",
       " 2.000692,\n",
       " 2.0056264,\n",
       " 1.9337138,\n",
       " 1.8692511,\n",
       " 1.946561,\n",
       " 2.0278847,\n",
       " 2.0175216,\n",
       " 1.9925598,\n",
       " 2.0043364,\n",
       " 1.9925562,\n",
       " 2.0323844,\n",
       " 2.0070624,\n",
       " 1.9514256,\n",
       " 1.908775,\n",
       " 1.9642814,\n",
       " 1.9198738,\n",
       " 2.008022,\n",
       " 2.014621,\n",
       " 1.8692366,\n",
       " 1.8537427,\n",
       " 1.820876,\n",
       " 1.9602733,\n",
       " 1.8769107,\n",
       " 2.0389369,\n",
       " 1.9478353,\n",
       " 2.0452032,\n",
       " 2.0392804,\n",
       " 1.9887108,\n",
       " 2.14121,\n",
       " 2.0844917,\n",
       " 1.9155792,\n",
       " 1.9559959,\n",
       " 2.0974307,\n",
       " 2.0023909,\n",
       " 2.121052,\n",
       " 1.9817909,\n",
       " 2.0205867,\n",
       " 1.9217713,\n",
       " 2.0061977,\n",
       " 2.0031626,\n",
       " 1.9977466,\n",
       " 2.039326,\n",
       " 1.9037764,\n",
       " 2.057299,\n",
       " 2.0128627,\n",
       " 2.0696733,\n",
       " 1.9287692,\n",
       " 1.9311867,\n",
       " 1.9073069,\n",
       " 1.9085217,\n",
       " 1.9242402,\n",
       " 1.9847202,\n",
       " 2.132789,\n",
       " 1.9881846,\n",
       " 1.9968249,\n",
       " 1.963435,\n",
       " 1.9785485,\n",
       " 1.9716562,\n",
       " 1.9387648,\n",
       " 1.9684069,\n",
       " 1.9471531,\n",
       " 1.9875745,\n",
       " 2.0024893,\n",
       " 1.9064872,\n",
       " 1.9099762,\n",
       " 2.0776858,\n",
       " 2.0346327,\n",
       " 2.0109644,\n",
       " 1.8692268,\n",
       " 2.0536504,\n",
       " 1.9707829,\n",
       " 1.8499373,\n",
       " 1.9934762,\n",
       " 1.9161828,\n",
       " 1.9269496,\n",
       " 1.9062923,\n",
       " 2.029244,\n",
       " 1.9182937,\n",
       " 1.8185133,\n",
       " 1.8098353,\n",
       " 1.9299195,\n",
       " 1.8315862,\n",
       " 2.0562556,\n",
       " 1.9282919,\n",
       " 1.9439473,\n",
       " 1.869147,\n",
       " 1.93648,\n",
       " 1.860128,\n",
       " 1.8587918,\n",
       " 1.9246575,\n",
       " 1.9398582,\n",
       " 2.0570529,\n",
       " 2.0368333,\n",
       " 1.9161353,\n",
       " 1.9037796,\n",
       " 1.9721555,\n",
       " 1.8049608,\n",
       " 2.0017426,\n",
       " 1.895673,\n",
       " 1.888529,\n",
       " 2.0743554,\n",
       " 1.9021564,\n",
       " 1.9074154,\n",
       " 1.8379868,\n",
       " 1.9909859,\n",
       " 1.7406468,\n",
       " 1.7792019,\n",
       " 1.7869698,\n",
       " 1.8096299,\n",
       " 2.0060122,\n",
       " 1.9695612,\n",
       " 1.9807912,\n",
       " 1.9095865,\n",
       " 1.7134137,\n",
       " 1.8005229,\n",
       " 1.8473723,\n",
       " 1.8982186,\n",
       " 1.780254,\n",
       " 1.7858627,\n",
       " 1.9265805,\n",
       " 1.8869517,\n",
       " 1.9181131,\n",
       " 1.9314855,\n",
       " 1.9823611,\n",
       " 1.9917722,\n",
       " 1.9642864,\n",
       " 2.0200982,\n",
       " 1.7829791,\n",
       " 1.958683,\n",
       " 1.9339997,\n",
       " 1.912888,\n",
       " 1.8491267,\n",
       " 1.7401108,\n",
       " 1.9173405,\n",
       " 1.984823,\n",
       " 2.0456376,\n",
       " 1.8554553,\n",
       " 1.9762495,\n",
       " 1.8702812,\n",
       " 1.8873364,\n",
       " 1.8599358,\n",
       " 1.8756522,\n",
       " 1.8498721,\n",
       " 2.0479712,\n",
       " 1.9313011,\n",
       " 1.8211161,\n",
       " 1.9858648,\n",
       " 1.9858612,\n",
       " 1.792495,\n",
       " 1.7334961,\n",
       " 1.9495397,\n",
       " 1.8445008,\n",
       " 1.8223224,\n",
       " 1.8196294,\n",
       " 2.042348,\n",
       " 1.9292417,\n",
       " 1.9269487,\n",
       " 1.9223628,\n",
       " 1.924721,\n",
       " 1.7879092,\n",
       " 1.8484752,\n",
       " 1.7383138,\n",
       " 2.0308554,\n",
       " 2.0470023,\n",
       " 1.8801736,\n",
       " 1.840831,\n",
       " 1.9320457,\n",
       " 1.7629534,\n",
       " 1.9062481,\n",
       " 1.8505251,\n",
       " 1.8929188,\n",
       " 1.8213288,\n",
       " 1.8869225,\n",
       " 1.9023317,\n",
       " 1.8808345,\n",
       " 2.021872,\n",
       " 1.8303878,\n",
       " 1.8408478,\n",
       " 1.8964857,\n",
       " 1.8468558,\n",
       " 1.8294454,\n",
       " 2.0293317,\n",
       " 1.7953681,\n",
       " 1.8605483,\n",
       " 1.9206502,\n",
       " 1.9256047,\n",
       " 1.9316188,\n",
       " 1.9222754,\n",
       " 1.9595779,\n",
       " 1.7510062,\n",
       " 1.9251782,\n",
       " 1.956665,\n",
       " 1.896076,\n",
       " 1.8497765,\n",
       " 1.8764018,\n",
       " 1.6798831,\n",
       " 1.9249068,\n",
       " 1.8802147,\n",
       " 1.8342863,\n",
       " 1.7296643,\n",
       " 1.9084077,\n",
       " 1.8916848,\n",
       " 1.9854932,\n",
       " 1.8822535,\n",
       " 1.959016,\n",
       " 1.9379036,\n",
       " 1.9180493,\n",
       " 1.8647468,\n",
       " 1.8051035,\n",
       " 1.8582519,\n",
       " 1.8924719,\n",
       " 1.8243307,\n",
       " 1.8334767,\n",
       " 1.8891556,\n",
       " 1.7964752,\n",
       " 1.8272359,\n",
       " 1.9125011,\n",
       " 1.7762847,\n",
       " 1.8980482,\n",
       " 1.8315778,\n",
       " 1.6915627,\n",
       " 2.0149665,\n",
       " 2.0161872,\n",
       " 1.8176458,\n",
       " 1.7267468,\n",
       " 1.876144,\n",
       " 1.826739,\n",
       " 1.8182994,\n",
       " 1.6782862,\n",
       " 1.8423649,\n",
       " 1.7913574,\n",
       " 1.7291187,\n",
       " 1.8803033,\n",
       " 1.6732692,\n",
       " 1.9213349,\n",
       " 1.8057445,\n",
       " 1.8944377,\n",
       " 1.8727323,\n",
       " 1.7554008,\n",
       " 1.8970435,\n",
       " 1.857545,\n",
       " 1.983372,\n",
       " 1.8311929,\n",
       " 1.9931008,\n",
       " 1.7379949,\n",
       " 1.8045536,\n",
       " 1.7788745,\n",
       " 1.8270441,\n",
       " 1.8654819,\n",
       " 1.8213974,\n",
       " 1.9817207,\n",
       " 1.7668495,\n",
       " 1.7775363,\n",
       " 1.7479701,\n",
       " 1.7581667,\n",
       " 1.7733467,\n",
       " 1.7762283,\n",
       " 1.8711884,\n",
       " 1.9936994,\n",
       " 1.7844325,\n",
       " 1.8303945,\n",
       " 1.9195071,\n",
       " 1.7226678,\n",
       " 1.8885536,\n",
       " 1.8585635,\n",
       " 1.8648187,\n",
       " 1.9497237,\n",
       " 1.7429767,\n",
       " 1.9326068,\n",
       " 1.7451112,\n",
       " 1.8463571,\n",
       " 1.7774438,\n",
       " 1.6341956,\n",
       " 1.8821378,\n",
       " 1.8460561,\n",
       " 1.8409177,\n",
       " 1.8293706,\n",
       " 1.9239606,\n",
       " 1.9025915,\n",
       " 1.9274158,\n",
       " 2.0255072,\n",
       " 1.8576045,\n",
       " 1.8545022,\n",
       " 1.8197823,\n",
       " 1.8103083,\n",
       " 1.8298268,\n",
       " 1.7951595,\n",
       " 1.7244102,\n",
       " 1.8898374,\n",
       " 1.9465088,\n",
       " 1.7643629,\n",
       " 1.8320979,\n",
       " 1.838863,\n",
       " 1.7335052,\n",
       " 1.7750536,\n",
       " 1.7362518,\n",
       " 1.7920759,\n",
       " 1.8337657,\n",
       " 1.791612,\n",
       " 1.7406911,\n",
       " 1.789377,\n",
       " 1.8054659,\n",
       " 1.8181053,\n",
       " 1.7294538,\n",
       " 1.8961048,\n",
       " 1.9363159,\n",
       " 1.6817454,\n",
       " 1.9121455,\n",
       " 1.8162826,\n",
       " 1.8929317,\n",
       " 1.8257389,\n",
       " 1.7490741,\n",
       " 1.7557411,\n",
       " 1.8289604,\n",
       " 1.6794999,\n",
       " 1.7320849,\n",
       " 1.8362478,\n",
       " 1.8384118,\n",
       " 1.8584175,\n",
       " 1.8258654,\n",
       " 1.8029969,\n",
       " 1.7031587,\n",
       " 1.6674064,\n",
       " 1.770762,\n",
       " 1.9202778,\n",
       " 1.6571745,\n",
       " 1.710491,\n",
       " 1.6461514,\n",
       " 1.7422988,\n",
       " 1.6317534,\n",
       " 2.075626,\n",
       " 1.9047953,\n",
       " 1.7329916,\n",
       " 1.6268243,\n",
       " 1.4965212,\n",
       " 1.6734812,\n",
       " 1.8289472,\n",
       " 1.9075185,\n",
       " 1.9258813,\n",
       " 2.002117,\n",
       " 1.8396431,\n",
       " 1.7736354,\n",
       " 1.8435633,\n",
       " 1.7769846,\n",
       " 1.8200457,\n",
       " 1.7728626,\n",
       " 1.692665,\n",
       " 1.922717,\n",
       " 1.8683838,\n",
       " 1.8466889,\n",
       " 1.9015884,\n",
       " 1.8896031,\n",
       " 1.6996689,\n",
       " 1.7273998,\n",
       " 1.7486649,\n",
       " 1.731336,\n",
       " 1.6919267,\n",
       " 1.7557789,\n",
       " 1.7849355,\n",
       " 1.8091292,\n",
       " 1.8464948,\n",
       " 1.9024117,\n",
       " 1.842712,\n",
       " 1.8810089,\n",
       " 1.6364319,\n",
       " 1.7550967,\n",
       " 1.831707,\n",
       " 1.771908,\n",
       " 1.8053147,\n",
       " 1.842932,\n",
       " 1.7770222,\n",
       " 1.7385354,\n",
       " 1.8497711,\n",
       " 1.7056295,\n",
       " 1.8204732,\n",
       " 1.8625587,\n",
       " 1.699442,\n",
       " 1.7207592,\n",
       " 1.8147967,\n",
       " 1.9223572,\n",
       " 1.8569266,\n",
       " 1.8501427,\n",
       " 1.814886,\n",
       " 1.7552619,\n",
       " 1.7557466,\n",
       " 1.6297872,\n",
       " 1.8177352,\n",
       " 1.7526687,\n",
       " 1.7277044,\n",
       " 1.8144681,\n",
       " 1.6866673,\n",
       " 1.7661031,\n",
       " 1.744526,\n",
       " 1.8458198,\n",
       " 1.7719381,\n",
       " 1.7452843,\n",
       " 1.7309248,\n",
       " 1.9420516,\n",
       " 1.6507367,\n",
       " 1.7573494,\n",
       " 1.6612456,\n",
       " 1.7304194,\n",
       " 1.791926,\n",
       " 1.7306838,\n",
       " 1.7487891,\n",
       " 1.8193423,\n",
       " 1.7364308,\n",
       " 1.6553538,\n",
       " 1.7083768,\n",
       " 1.6326512,\n",
       " 1.7698362,\n",
       " 1.8543972,\n",
       " 1.6302181,\n",
       " 1.7718178,\n",
       " 1.739905,\n",
       " 1.7611849,\n",
       " 1.7272657,\n",
       " 1.7935497,\n",
       " 1.7470375,\n",
       " 1.5267314,\n",
       " 1.6534413,\n",
       " 1.7078474,\n",
       " 1.6785507,\n",
       " 1.7671444,\n",
       " 1.813762,\n",
       " 1.6988442,\n",
       " 1.8271844,\n",
       " 1.7613384,\n",
       " 1.7303592,\n",
       " 1.5826315,\n",
       " 1.7807204,\n",
       " 1.6982256,\n",
       " 1.7400911,\n",
       " 1.6963903,\n",
       " 1.7175666,\n",
       " 1.8037369,\n",
       " 1.808392,\n",
       " 1.6762046,\n",
       " 1.6424026,\n",
       " 1.7858543,\n",
       " 1.7170359,\n",
       " 1.6453211,\n",
       " 1.7422059,\n",
       " 1.8537285,\n",
       " 1.7813433,\n",
       " 1.8426503,\n",
       " 1.6722666,\n",
       " 1.5227697,\n",
       " 1.6358492,\n",
       " 1.5995423,\n",
       " 1.6898206,\n",
       " 1.656903,\n",
       " 1.7599747,\n",
       " 1.5966176,\n",
       " 1.6109517,\n",
       " 1.7420495,\n",
       " 1.8709073,\n",
       " 1.6764903,\n",
       " 1.6736748,\n",
       " 1.7492009,\n",
       " 1.6545469,\n",
       " 1.7634219,\n",
       " 1.8079517,\n",
       " 1.7715544,\n",
       " 1.7633595,\n",
       " 1.6003083,\n",
       " 1.693517,\n",
       " 1.7375759,\n",
       " 1.8272573,\n",
       " 1.6908126,\n",
       " 1.6015338,\n",
       " 1.7140999,\n",
       " 1.767901,\n",
       " 1.8152648,\n",
       " 1.5425323,\n",
       " 1.7788373,\n",
       " 1.6415159,\n",
       " 1.6750778,\n",
       " 1.6304071,\n",
       " 1.6754783,\n",
       " 1.8103809,\n",
       " 1.7306145,\n",
       " 1.8834155,\n",
       " 1.6906943,\n",
       " 1.7309949,\n",
       " 1.5104523,\n",
       " 1.7278761,\n",
       " 1.7176527,\n",
       " 1.6938262,\n",
       " 1.58679,\n",
       " 1.7074912,\n",
       " 1.6999936,\n",
       " 1.6050485,\n",
       " 1.7170635,\n",
       " 1.7172668,\n",
       " 1.6894649,\n",
       " 1.7250979,\n",
       " 1.6763355,\n",
       " 1.8325094,\n",
       " 1.7704594,\n",
       " 1.86252,\n",
       " 1.9142503,\n",
       " 1.7368348,\n",
       " 1.7641226,\n",
       " 1.8626801,\n",
       " 1.7395664,\n",
       " 1.6747627,\n",
       " 1.7035019,\n",
       " 1.7367327,\n",
       " 1.8381002,\n",
       " 1.8029351,\n",
       " 1.6413995,\n",
       " 1.6678451,\n",
       " 1.7807988,\n",
       " 1.5940803,\n",
       " 1.5785996,\n",
       " 1.644435,\n",
       " 1.5972314,\n",
       " 1.6746897,\n",
       " 1.8519361,\n",
       " 1.8169125,\n",
       " 1.769295,\n",
       " 1.6178535,\n",
       " 1.5811688,\n",
       " 1.7821972,\n",
       " 1.7091639,\n",
       " 1.7205844,\n",
       " 1.7392699,\n",
       " 1.8828708,\n",
       " 1.8651288,\n",
       " 1.8060455,\n",
       " 1.66874,\n",
       " 1.6273465,\n",
       " 1.7914975,\n",
       " 1.6666287,\n",
       " 1.6882449,\n",
       " 1.7695187,\n",
       " 1.7371876,\n",
       " 1.6564013,\n",
       " 1.8369545,\n",
       " 1.6720551,\n",
       " 1.5942159,\n",
       " 1.7484757,\n",
       " 1.7530856,\n",
       " 1.6759754,\n",
       " 1.8284202,\n",
       " 1.5925229,\n",
       " 1.5259633,\n",
       " 1.6960572,\n",
       " 1.6781046,\n",
       " 1.7046552,\n",
       " 1.7841841,\n",
       " 1.6873991,\n",
       " 1.7380275,\n",
       " 1.6718428,\n",
       " 1.5445781,\n",
       " 1.8054072,\n",
       " 1.5444682,\n",
       " 1.6578077,\n",
       " 1.6866525,\n",
       " 1.548435,\n",
       " 1.4715302,\n",
       " 1.7314649,\n",
       " 1.6005839,\n",
       " 1.8019569,\n",
       " 1.71162,\n",
       " 1.7272146,\n",
       " 1.6584803,\n",
       " 1.5810926,\n",
       " 1.7619659,\n",
       " 1.6428802,\n",
       " 1.7975713,\n",
       " 1.7006892,\n",
       " 1.5224242,\n",
       " 1.6002316,\n",
       " 1.6059284,\n",
       " 1.7091172,\n",
       " 1.7709656,\n",
       " 1.7423722,\n",
       " 1.724563,\n",
       " 1.7145528,\n",
       " 1.7131693,\n",
       " 1.7215565,\n",
       " 1.7106395,\n",
       " 1.5663937,\n",
       " 1.7813817,\n",
       " 1.779174,\n",
       " 1.6004335,\n",
       " 1.7914424,\n",
       " 1.6379316,\n",
       " 1.538324,\n",
       " 1.7300211,\n",
       " 1.6271226,\n",
       " 1.4261321,\n",
       " 1.6760544,\n",
       " 1.6537446,\n",
       " 1.6702675,\n",
       " 1.6943604,\n",
       " 1.8251599,\n",
       " 1.557872,\n",
       " 1.6953291,\n",
       " 1.586777,\n",
       " 1.4634864,\n",
       " 1.5646527,\n",
       " 1.5303129,\n",
       " 1.6715245,\n",
       " 1.5864042,\n",
       " 1.8144157,\n",
       " 1.6030918,\n",
       " 1.5444692,\n",
       " 1.5848678,\n",
       " 1.6167616,\n",
       " 1.6116089,\n",
       " 1.6116804,\n",
       " 1.6568706,\n",
       " 1.7295276,\n",
       " 1.5586998,\n",
       " 1.5226115,\n",
       " 1.6048503,\n",
       " 1.7900139,\n",
       " 1.6694453,\n",
       " 1.6203331,\n",
       " 1.6567711,\n",
       " 1.7063527,\n",
       " 1.5659516,\n",
       " 1.5824376,\n",
       " 1.5068996,\n",
       " 1.7342869,\n",
       " 1.5857776,\n",
       " 1.6679301,\n",
       " 1.5160487,\n",
       " 1.4322774,\n",
       " 1.6497748,\n",
       " 1.6005071,\n",
       " 1.6010951,\n",
       " 1.6691744,\n",
       " 1.5682542,\n",
       " 1.7579378,\n",
       " 1.6883774,\n",
       " 1.5288478,\n",
       " 1.5043551,\n",
       " 1.503593,\n",
       " 1.7521989,\n",
       " 1.7677438,\n",
       " 1.7265239,\n",
       " 1.640581,\n",
       " 1.706472,\n",
       " 1.6172632,\n",
       " 1.7599597,\n",
       " 1.435718,\n",
       " 1.5219961,\n",
       " 1.4555581,\n",
       " 1.5495706,\n",
       " 1.6119578,\n",
       " 1.7187248,\n",
       " 1.4078193,\n",
       " 1.6356579,\n",
       " 1.6422007,\n",
       " 1.5958797,\n",
       " 1.6551534,\n",
       " 1.5870149,\n",
       " 1.5838032]"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(len(loss_hist))\n",
    "loss_hist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'loss_hist' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-23-5c2f5f1c2ad9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;31m# draw only dots without connecting lines\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss_hist\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'o'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mxlabel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Batch #'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mylabel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Loss [entropy]'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'loss_hist' is not defined"
     ]
    }
   ],
   "source": [
    "\n",
    "# draw connecting lines\n",
    "# plt.plot(loss_hist)\n",
    "\n",
    "# draw only dots without connecting lines\n",
    "plt.plot(loss_hist, 'o')\n",
    "plt.xlabel('Batch #')\n",
    "plt.ylabel('Loss [entropy]')\n",
    "plt.savefig('loss_batch{}.png'.format(batch_size))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kKpOlHPLqEgl"
   },
   "source": [
    "### Variables and optimizers\n",
    "\n",
    "`tf.Variable` objects store mutable `tf.Tensor`-like values accessed during\n",
    "training to make automatic differentiation easier. \n",
    "\n",
    "The collections of variables can be encapsulated into layers or models, along with methods that operate on them. See [Custom Keras layers and models](./keras/custom_layers_and_models.ipynb) for details. The main difference between layers and models is that models add methods like  `Model.fit`, `Model.evaluate`, and `Model.save`.\n",
    "\n",
    "For example, the automatic differentiation example above\n",
    "can be rewritten:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Linear(tf.keras.Model):\n",
    "    def __init__(self):\n",
    "        super(Linear, self).__init__()\n",
    "        self.W = tf.Variable(5., name='weight')\n",
    "        self.B = tf.Variable(1., name='bias')\n",
    "    def call(self, inputs):\n",
    "        return inputs*self.W + self.B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A toy dataset of points around 4 * x + 5\n",
    "NUM_EXAMPLES = 1000\n",
    "training_inputs = tf.random.normal([NUM_EXAMPLES])\n",
    "noise = tf.random.normal([NUM_EXAMPLES])\n",
    "training_outputs = training_inputs * 4 + 5\n",
    "\n",
    "def loss(model, inputs, targets):\n",
    "    error = model(inputs) - targets\n",
    "    return tf.reduce_mean(tf.square(error))\n",
    "\n",
    "def grad(model, inputs, targets):\n",
    "    with tf.GradientTape() as tape:\n",
    "        loss_val = loss(model, inputs, targets)\n",
    "    return tape.gradient(loss_val, [model.W, model.B])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Q7x1CDurl3IG"
   },
   "source": [
    "Next:\n",
    "\n",
    "1. Create the model.\n",
    "2. The Derivatives of a loss function with respect to model parameters.\n",
    "3. A strategy for updating the variables based on the derivatives."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial loss: 16.939\n",
      "Loss at step 000: 16.270662\n",
      "Loss at step 020: 7.273061\n",
      "Loss at step 040: 3.251109\n",
      "Loss at step 060: 1.453277\n",
      "Loss at step 080: 0.649632\n",
      "Loss at step 100: 0.290394\n",
      "Loss at step 120: 0.129811\n",
      "Loss at step 140: 0.058028\n",
      "Loss at step 160: 0.025940\n",
      "Loss at step 180: 0.011596\n"
     ]
    }
   ],
   "source": [
    "model = Linear()\n",
    "optimizer = tf.keras.optimizers.SGD(learning_rate=0.01)\n",
    "\n",
    "print(\"Initial loss: {:.3f}\".format( loss(model, training_inputs, training_outputs) ))\n",
    "\n",
    "steps = 200\n",
    "for i in range(steps):\n",
    "    grads = grad(model, training_inputs, training_outputs)\n",
    "    optimizer.apply_gradients( zip(grads, [model.W, model.B]) )\n",
    "    \n",
    "    if i%20 == 0:\n",
    "        print(\"Loss at step {:03d}: {:3f}\".format(i, loss(model, training_inputs, training_outputs) ) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final loss : 0.005\n"
     ]
    }
   ],
   "source": [
    "print(\"Final loss : {:.3f}\".format( loss(model, training_inputs, training_outputs) ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Weights - W = 4.018618583679199, B = 4.928799152374268\n"
     ]
    }
   ],
   "source": [
    "print(\"Weights - W = {}, B = {}\".format(model.W.numpy(), model.B.numpy() ))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rPjb8nRWqEgr"
   },
   "source": [
    "Note: Variables persist until the last reference to the python object\n",
    "is removed, and then the variable is deleted."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "scMjg6L6qEgv"
   },
   "source": [
    "### Object-based saving\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Y-0ZcCcjwkux"
   },
   "source": [
    "A `tf.keras.Model` includes a convenient `save_weights` method allowing you to easily create a checkpoint: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: model.full/assets\n"
     ]
    }
   ],
   "source": [
    "model.save_weights('model.weights')\n",
    "model.save('model.full')\n",
    "loaded_model = model.load_weights('model.weights')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(5,), dtype=float32, numpy=\n",
       "array([-0.87969667,  1.1847961 , -0.11015344,  0.07763214,  0.46290445],\n",
       "      dtype=float32)>"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_inputs[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(5,), dtype=float32, numpy=\n",
       "array([1.4812133, 9.739184 , 4.5593863, 5.3105288, 6.851618 ],\n",
       "      dtype=float32)>"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tmp = training_inputs*4 + 5 \n",
    "tmp[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1.3936338],\n",
       "       [9.6900425],\n",
       "       [4.4861345],\n",
       "       [5.240773 ],\n",
       "       [6.789036 ]], dtype=float32)"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.predict(training_inputs[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2EfTjWV_wEng"
   },
   "source": [
    "Using `tf.train.Checkpoint` you can take full control over this process.\n",
    "\n",
    "This section is an abbreviated version of the [guide to training checkpoints](./checkpoint.ipynb).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<tf.Variable 'Variable:0' shape=() dtype=float32, numpy=10.0>\n",
      "<tf.Variable 'Variable:0' shape=() dtype=float32, numpy=5.0>\n",
      "<tf.Variable 'Variable:0' shape=() dtype=float32, numpy=101.0>\n",
      "<tf.Variable 'Variable:0' shape=() dtype=float32, numpy=5.0>\n"
     ]
    }
   ],
   "source": [
    "v = tf.Variable(10.)\n",
    "print(v)\n",
    "chkpt = tf.train.Checkpoint(v=v)\n",
    "# Assign a new value to the variables and save.\n",
    "v.assign(5.)\n",
    "print(v)\n",
    "chkpt_path = \"./ckpt/\"\n",
    "chkpt.save(chkpt_path)\n",
    "v.assign(101.)\n",
    "print(v)\n",
    "chkpt.restore(tf.train.latest_checkpoint(chkpt_path))\n",
    "print(v)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vbFnP-yLqEgx"
   },
   "source": [
    "To save and load models, `tf.train.Checkpoint` stores the internal state of objects,\n",
    "without requiring hidden variables. To record the state of a `model`,\n",
    "an `optimizer`, and a global step, pass them to a `tf.train.Checkpoint`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.training.tracking.util.CheckpointLoadStatus at 0x7f0ef7d937f0>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Conv2D(16, [3,3], activation='relu')\n",
    "    , tf.keras.layers.GlobalAveragePooling2D()\n",
    "    , tf.keras.layers.Dense(10)\n",
    "])\n",
    "\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=0.001)\n",
    "chkpt2_dir = \"./ckpt2/\"\n",
    "chkpt2 = tf.train.Checkpoint(optimizer=optimizer, model=model)\n",
    "chkpt2.save(chkpt2_dir)\n",
    "chkpt2.restore(tf.train.latest_checkpoint(chkpt2_dir))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "R-ITwkBCF6GJ"
   },
   "source": [
    "Note: In many training loops, variables are created after `tf.train.Checkpoint.restore` is called. These variables will be restored as soon as they are created, and assertions are available to ensure that a checkpoint has been fully loaded. See the [guide to training checkpoints](./checkpoint.ipynb) for details."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3yoD0VJ7qEg3"
   },
   "source": [
    "### Object-oriented metrics\n",
    "\n",
    "`tf.keras.metrics` are stored as objects. Update a metric by passing the new data to\n",
    "the callable, and retrieve the result using the `tf.keras.metrics.result` method,\n",
    "for example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(2.0, shape=(), dtype=float32)\n",
      "tf.Tensor(4.0, shape=(), dtype=float32)\n",
      "tf.Tensor(8.0, shape=(), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "m = tf.keras.metrics.Mean('loss')\n",
    "m(1)\n",
    "m(3)\n",
    "print(m.result())\n",
    "m([5,7])\n",
    "print(m.result())\n",
    "m.update_state([9,11,13,15])\n",
    "print(m.result())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aB8qWtT955pI"
   },
   "source": [
    "### Summaries and TensorBoard\n",
    "\n",
    "[TensorBoard](https://tensorflow.org/tensorboard) is a visualization tool for\n",
    "understanding, debugging and optimizing the model training process. It uses\n",
    "summary events that are written while executing the program.\n",
    "\n",
    "You can use `tf.summary` to record summaries of variable in eager execution.\n",
    "For example, to record summaries of `loss` once every 100 training steps:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "z6VInqhA6RH4"
   },
   "outputs": [],
   "source": [
    "logdir = \"./tb/\"\n",
    "writer = tf.summary.create_file_writer(logdir)\n",
    "\n",
    "steps = 1000\n",
    "with writer.as_default():  # or call writer.set_as_default() before the loop.\n",
    "  for i in range(steps):\n",
    "    step = i + 1\n",
    "    # Calculate loss with your real train function.\n",
    "    loss = 1 - 0.001 * step\n",
    "    if step % 100 == 0:\n",
    "      tf.summary.scalar('loss', loss, step=step)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_dir = './tb/'\n",
    "writer = tf.summary.create_file_writer(log_dir)\n",
    "\n",
    "steps = 1000\n",
    "# or call writer.set_as_default() before the loop.\n",
    "with writer.as_default():\n",
    "    for i in range(steps):\n",
    "        step = i+1\n",
    "        # Calculate loss with your real train function.\n",
    "        loss = 1 - 0.001 * step\n",
    "        if step%50 == 0:\n",
    "            tf.summary.scalar('loss', loss, step=step)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "08QQD2j36TaI"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "events.out.tfevents.1634495007.pop-os.5592.0.v2\r\n"
     ]
    }
   ],
   "source": [
    "!ls tb/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "NOTE: Using experimental fast data loading logic. To disable, pass\n",
      "    \"--load_fast=false\" and report issues on GitHub. More details:\n",
      "    https://github.com/tensorflow/tensorboard/issues/4784\n",
      "\n",
      "Serving TensorBoard on localhost; to expose to the network, use a proxy or pass --bind_all\n",
      "TensorBoard 2.6.0 at http://localhost:6006/ (Press CTRL+C to quit)\n",
      "^C\n"
     ]
    }
   ],
   "source": [
    "!tensorboard --logdir=."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xEL4yJe5qEhD"
   },
   "source": [
    "## Advanced automatic differentiation topics\n",
    "\n",
    "### Dynamic models\n",
    "\n",
    "`tf.GradientTape` can also be used in dynamic models. This example for a\n",
    "[backtracking line search](https://wikipedia.org/wiki/Backtracking_line_search)\n",
    "algorithm looks like normal NumPy code, except there are gradients and is\n",
    "differentiable, despite the complex control flow:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "L518n5dkqEhE"
   },
   "outputs": [],
   "source": [
    "def line_search_step(fn, init_x, rate=1.0):\n",
    "  with tf.GradientTape() as tape:\n",
    "    # Variables are automatically tracked.\n",
    "    # But to calculate a gradient from a tensor, you must `watch` it.\n",
    "    tape.watch(init_x)\n",
    "    value = fn(init_x)\n",
    "  grad = tape.gradient(value, init_x)\n",
    "  grad_norm = tf.reduce_sum(grad * grad)\n",
    "  init_value = value\n",
    "  while value > init_value - rate * grad_norm:\n",
    "    x = init_x - rate * grad\n",
    "    value = fn(x)\n",
    "    rate /= 2.0\n",
    "  return x, value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gieGOf_DqEhK"
   },
   "source": [
    "### Custom gradients\n",
    "\n",
    "Custom gradients are an easy way to override gradients. Within the forward function, define the gradient with respect to the\n",
    "inputs, outputs, or intermediate results. For example, here's an easy way to clip\n",
    "the norm of the gradients in the backward pass:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### @me - Refer this link to understand better -\n",
    "https://www.tensorflow.org/api_docs/python/tf/custom_gradient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "-OwwsWUAqEhK"
   },
   "outputs": [],
   "source": [
    "@tf.custom_gradient\n",
    "def clip_gradient_by_norm(x, norm):\n",
    "  y = tf.identity(x)\n",
    "  def grad_fn(dresult):\n",
    "    return [tf.clip_by_norm(dresult, norm), None]\n",
    "  return y, grad_fn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JPLDHkF_qEhN"
   },
   "source": [
    "Custom gradients are commonly used to provide a numerically stable gradient for a\n",
    "sequence of operations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "24WiLROnqEhO"
   },
   "outputs": [],
   "source": [
    "def log1pexp(x):\n",
    "  return tf.math.log(1 + tf.exp(x))\n",
    "\n",
    "def grad_log1pexp(x):\n",
    "  with tf.GradientTape() as tape:\n",
    "    tape.watch(x)\n",
    "    value = log1pexp(x)\n",
    "  return tape.gradient(value, x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "n8fq69r9-B-c"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# The gradient computation works fine at x = 0.\n",
    "grad_log1pexp(tf.constant(0.)).numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "_VFSU0mG-FSp"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "nan"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# However, x = 100 fails because of numerical instability.\n",
    "grad_log1pexp(tf.constant(100.)).numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-VcTR34rqEhQ"
   },
   "source": [
    "Here, the `log1pexp` function can be analytically simplified with a custom\n",
    "gradient. The implementation below reuses the value for `tf.exp(x)` that is\n",
    "computed during the forward pass—making it more efficient by eliminating\n",
    "redundant calculations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "Q7nvfx_-qEhS"
   },
   "outputs": [],
   "source": [
    "@tf.custom_gradient\n",
    "def log1pexp(x):\n",
    "  e = tf.exp(x)\n",
    "  def grad(dy):\n",
    "    return dy * (1 - 1 / (1 + e))\n",
    "  return tf.math.log(1 + e), grad\n",
    "\n",
    "def grad_log1pexp(x):\n",
    "  with tf.GradientTape() as tape:\n",
    "    tape.watch(x)\n",
    "    value = log1pexp(x)\n",
    "  return tape.gradient(value, x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "5gHPKMfl-Kge"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# As before, the gradient computation works fine at x = 0.\n",
    "grad_log1pexp(tf.constant(0.)).numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "u38MOfz3-MDE"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# And the gradient computation also works at x = 100.\n",
    "grad_log1pexp(tf.constant(100.)).numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rnZXjfQzqEhV"
   },
   "source": [
    "## Performance\n",
    "\n",
    "Computation is automatically offloaded to GPUs during eager execution. If you\n",
    "want control over where a computation runs you can enclose it in a\n",
    "`tf.device('/gpu:0')` block (or the CPU equivalent):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Net time taken by CPU = 0.0004818439483642578\n",
      "Net time taken by GPU = 0.0016014575958251953\n",
      "CPU/GPU time taken ratio = 0.3008783683191901\n",
      "Net time taken by CPU = 0.5290358066558838\n",
      "Net time taken by GPU = 0.024556398391723633\n",
      "CPU/GPU time taken ratio = 21.543705156460867\n",
      "Net time taken by CPU = 1.0470123291015625\n",
      "Net time taken by GPU = 0.046486616134643555\n",
      "CPU/GPU time taken ratio = 22.52287682263218\n",
      "Net time taken by CPU = 1.5510294437408447\n",
      "Net time taken by GPU = 0.0791635513305664\n",
      "CPU/GPU time taken ratio = 19.592721873531787\n",
      "Net time taken by CPU = 2.0943856239318848\n",
      "Net time taken by GPU = 0.09776425361633301\n",
      "CPU/GPU time taken ratio = 21.422816074995183\n",
      "Net time taken by CPU = 2.626594066619873\n",
      "Net time taken by GPU = 0.13166570663452148\n",
      "CPU/GPU time taken ratio = 19.94896115137095\n",
      "Net time taken by CPU = 3.142956018447876\n",
      "Net time taken by GPU = 0.1515052318572998\n",
      "CPU/GPU time taken ratio = 20.74486788290039\n",
      "Net time taken by CPU = 3.625814437866211\n",
      "Net time taken by GPU = 0.17595243453979492\n",
      "CPU/GPU time taken ratio = 20.60678755226979\n",
      "Net time taken by CPU = 4.277359247207642\n",
      "Net time taken by GPU = 0.20476746559143066\n",
      "CPU/GPU time taken ratio = 20.888861591627013\n",
      "Net time taken by CPU = 4.717179298400879\n",
      "Net time taken by GPU = 0.25023531913757324\n",
      "CPU/GPU time taken ratio = 18.850973214566444\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7f0e6c674640>]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import time\n",
    "def calc(tensor, steps):\n",
    "    # TensorFlow initializes a GPU the first time it's used, exclude from timing.\n",
    "    tensor2 = tf.matmul(tensor, tensor)\n",
    "    \n",
    "    t0 = time.time()\n",
    "    for i in range(steps):\n",
    "        tensor2 = tf.matmul(tensor,tensor)\n",
    "        \n",
    "    # tf.matmul can return before completing the matrix multiplication\n",
    "    # (e.g., can return after enqueing the operation on a CUDA stream).\n",
    "    # The x.numpy() call below will ensure that all enqueued operations\n",
    "    # have completed (and will also copy the result to host memory,\n",
    "    # so we're including a little more than just the matmul operation\n",
    "    # time).\n",
    "    _ = tensor.numpy()\n",
    "    \n",
    "    t1 = time.time() - t0\n",
    "    return t1\n",
    "\n",
    "shape = (1000, 1000)\n",
    "steps = 1000\n",
    "cpu_gpu_ratio_lst = []\n",
    "# one can also add a for loop and draw plots by varying the range of values shape and steps can take\n",
    "for step in range(0, steps, 100):\n",
    "    with tf.device('/cpu:0'):\n",
    "        cpu_time = calc(tf.random.normal(shape), step)\n",
    "        print(\"Net time taken by CPU = {}\".format( cpu_time ))\n",
    "\n",
    "    if tf.config.list_physical_devices('GPU'):\n",
    "        with tf.device('/gpu:0'):\n",
    "            gpu_time = calc(tf.random.normal(shape), step)\n",
    "            print(\"Net time taken by GPU = {}\".format( gpu_time ))\n",
    "            \n",
    "        cpu_gpu_ratio = cpu_time/gpu_time\n",
    "        print(\"CPU/GPU time taken ratio = {}\".format(cpu_time/gpu_time) )\n",
    "        cpu_gpu_ratio_lst.append(cpu_gpu_ratio)\n",
    "    else:\n",
    "        print(\"No GPU available\")\n",
    "    \n",
    "plt.plot(cpu_gpu_ratio_lst)\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RLw3IS7UqEhe"
   },
   "source": [
    "A `tf.Tensor` object can be copied to a different device to execute its\n",
    "operations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "if tf.config.list_physical_devices('GPU'):\n",
    "    x = tf.random.normal([10, 10])\n",
    "    \n",
    "    x_gpu0 = x.gpu()\n",
    "    x_cpu = x.cpu()\n",
    "    \n",
    "    # runs on cpu\n",
    "    _ = tf.matmul(x_cpu, x_cpu)\n",
    "    # runs on gpu\n",
    "    _ = tf.matmul(x_gpu0, x_gpu0)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oA_qaII3-p6c"
   },
   "source": [
    "### Benchmarks\n",
    "\n",
    "For compute-heavy models, such as\n",
    "[ResNet50](https://github.com/tensorflow/tensorflow/tree/master/tensorflow/python/eager/benchmarks/resnet50)\n",
    "training on a GPU, eager execution performance is comparable to `tf.function` execution.\n",
    "But this gap grows larger for models with less computation and there is work to\n",
    "be done for optimizing hot code paths for models with lots of small operations.\n",
    "\n",
    "## Work with functions\n",
    "\n",
    "While eager execution makes development and debugging more interactive,\n",
    "TensorFlow 1.x style graph execution has advantages for distributed training, performance\n",
    "optimizations, and production deployment. To bridge this gap, TensorFlow 2.0 introduces `function`s via the `tf.function` API. For more information, see the [tf.function](./function.ipynb) guide."
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "eager.ipynb",
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
